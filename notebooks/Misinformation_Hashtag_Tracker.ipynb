]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Implementation Guide and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implementation_checklist():\n",
    "    \"\"\"Comprehensive implementation checklist for information access specialists\"\"\"\n",
    "    checklist = {\n",
    "        'technical_setup': {\n",
    "            'required_tools': [\n",
    "                'âœ“ Python environment with data science libraries',\n",
    "                'âœ“ API access for social media platforms',\n",
    "                'âœ“ Fact-checking database subscriptions',\n",
    "                'âœ“ Media verification tools (reverse search, metadata analysis)',\n",
    "                'âœ“ Network analysis and visualization software',\n",
    "                'âœ“ Secure data storage and processing infrastructure'\n",
    "            ],\n",
    "            'security_requirements': [\n",
    "                'âœ“ Encrypted data transmission and storage',\n",
    "                'âœ“ Access control and user authentication',\n",
    "                'âœ“ Regular security audits and updates',\n",
    "                'âœ“ Compliance with data protection regulations',\n",
    "                'âœ“ Incident response procedures'\n",
    "            ]\n",
    "        },\n",
    "        'operational_procedures': {\n",
    "            'daily_workflows': [\n",
    "                'âœ“ Morning briefing on overnight misinformation trends',\n",
    "                'âœ“ Priority queue review and task assignment',\n",
    "                'âœ“ Real-time monitoring dashboard checks',\n",
    "                'âœ“ Fact-check progress updates',\n",
    "                'âœ“ Platform coordination communications',\n",
    "                'âœ“ End-of-day reporting and handover'\n",
    "            ],\n",
    "            'quality_assurance': [\n",
    "                'âœ“ Peer review of all fact-check conclusions',\n",
    "                'âœ“ Source verification double-checking',\n",
    "                'âœ“ Regular accuracy audits',\n",
    "                'âœ“ Feedback incorporation processes',\n",
    "                'âœ“ Continuous improvement protocols'\n",
    "            ]\n",
    "        },\n",
    "        'team_structure': {\n",
    "            'core_roles': [\n",
    "                'âœ“ Lead Information Analyst',\n",
    "                'âœ“ Fact-Check Specialists (2-3)',\n",
    "                'âœ“ Technical/Data Analyst',\n",
    "                'âœ“ Platform Liaison Coordinator',\n",
    "                'âœ“ Training and Development Specialist'\n",
    "            ],\n",
    "            'extended_network': [\n",
    "                'âœ“ Subject matter experts (health, politics, technology)',\n",
    "                'âœ“ Legal counsel for complex cases',\n",
    "                'âœ“ Communication specialists',\n",
    "                'âœ“ Academic research partners',\n",
    "                'âœ“ International fact-checking collaborators'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return checklist\n",
    "\n",
    "def generate_best_practices_guide():\n",
    "    \"\"\"Generate comprehensive best practices guide\"\"\"\n",
    "    guide = {\n",
    "        'analysis_best_practices': {\n",
    "            'source_verification': [\n",
    "                'Always verify information with at least 3 independent sources',\n",
    "                'Check source credibility and potential conflicts of interest',\n",
    "                'Look for original sources rather than secondary reporting',\n",
    "                'Verify author credentials and expertise',\n",
    "                'Check publication date and temporal relevance'\n",
    "            ],\n",
    "            'media_authentication': [\n",
    "                'Use multiple reverse search engines for image verification',\n",
    "                'Analyze metadata for manipulation indicators',\n",
    "                'Verify geolocation and temporal context',\n",
    "                'Check for signs of digital manipulation',\n",
    "                'Consult with technical experts for complex cases'\n",
    "            ],\n",
    "            'network_analysis': [\n",
    "                'Map information flow patterns across platforms',\n",
    "                'Identify key influencers and amplifiers',\n",
    "                'Look for coordinated behavior indicators',\n",
    "                'Analyze timing and geographic patterns',\n",
    "                'Document evidence systematically'\n",
    "            ]\n",
    "        },\n",
    "        'response_best_practices': {\n",
    "            'rapid_response': [\n",
    "                'Establish clear escalation procedures',\n",
    "                'Maintain 24/7 monitoring capabilities',\n",
    "                'Pre-approved response templates for common scenarios',\n",
    "                'Direct communication channels with platform partners',\n",
    "                'Regular drill exercises for crisis scenarios'\n",
    "            ],\n",
    "            'public_communication': [\n",
    "                'Use clear, accessible language',\n",
    "                'Provide evidence and sources',\n",
    "                'Acknowledge uncertainty when appropriate',\n",
    "                'Focus on facts rather than attacking individuals',\n",
    "                'Coordinate messaging across organizations'\n",
    "            ]\n",
    "        },\n",
    "        'ethical_guidelines': {\n",
    "            'core_principles': [\n",
    "                'Accuracy and truthfulness above all',\n",
    "                'Transparency in methods and limitations',\n",
    "                'Respect for privacy and human dignity',\n",
    "                'Proportional response to threat level',\n",
    "                'Commitment to continuous learning and improvement'\n",
    "            ],\n",
    "            'conflict_resolution': [\n",
    "                'Acknowledge when evidence is inconclusive',\n",
    "                'Separate facts from opinions clearly',\n",
    "                'Provide context for controversial topics',\n",
    "                'Respect cultural and political sensitivities',\n",
    "                'Maintain political neutrality'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return guide\n",
    "\n",
    "def create_final_summary_report():\n",
    "    \"\"\"Create comprehensive final summary of all analyses\"\"\"\n",
    "    final_report = f\"\"\"\n",
    "    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    MISINFORMATION HASHTAG TRACKER - COMPREHENSIVE ANALYSIS REPORT\n",
    "    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \n",
    "    Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "    Analysis Period: Last 30 days\n",
    "    \n",
    "    ğŸ“Š EXECUTIVE DASHBOARD\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Total Posts Analyzed: {len(processed_data):,}\n",
    "    Unique Hashtags Tracked: {len(trending_hashtags)}\n",
    "    High-Risk Content: {len(processed_data[processed_data['misinformation_risk'] == 'high']):,} ({(len(processed_data[processed_data['misinformation_risk'] == 'high'])/len(processed_data)*100):.1f}%)\n",
    "    Platforms Monitored: {processed_data['platform'].nunique()}\n",
    "    Fact-Check Coverage: {(processed_data['fact_checked'].sum()/len(processed_data)*100):.1f}%\n",
    "    \n",
    "    ğŸ” KEY FINDINGS\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Top Misinformation Hashtag: #{trending_hashtags.iloc[0]['hashtag']}\n",
    "    Most Influential Node: #{influence_metrics.iloc[0]['hashtag']}\n",
    "    Coordinated Campaigns Detected: {len(potential_campaigns)}\n",
    "    Information Bottlenecks: {len(bottlenecks['critical_hashtags'])}\n",
    "    Active Infodemic Alerts: {len(emerging_infodemics)}\n",
    "    \n",
    "    ğŸš¨ THREAT ASSESSMENT\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Overall Threat Level: {threat_intelligence['executive_summary']['threat_level']}\n",
    "    Suspected Bot Accounts: {threat_intelligence['automation_threats']['suspected_bot_accounts']}\n",
    "    Platform Vulnerabilities: {len(threat_intelligence['platform_vulnerabilities'])} platforms assessed\n",
    "    Information Voids: {len(threat_intelligence['information_voids'])} topics identified\n",
    "    \n",
    "    âœ… VERIFICATION STATUS\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Posts Verified: {len(verification_results) if len(verification_results) > 0 else 0}\n",
    "    False Content Identified: {len(verification_results[verification_results['status'] == 'False']) if len(verification_results) > 0 else 0}\n",
    "    Disputed Claims: {len(verification_results[verification_results['status'] == 'Disputed']) if len(verification_results) > 0 else 0}\n",
    "    Recommended Actions: {len(verification_results[verification_results['recommended_action'] == 'Immediate Takedown']) if len(verification_results) > 0 else 0} takedowns\n",
    "    \n",
    "    ğŸ“ˆ DIGITAL LITERACY ASSESSMENT\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Population Literacy Score: {sample_assessment['overall_score']:.2f}/1.0\n",
    "    Primary Improvement Areas: {', '.join(sample_assessment['recommendations'][:2]) if sample_assessment['recommendations'] else 'None identified'}\n",
    "    Training Modules Available: {len(training_system.training_modules)}\n",
    "    Certification Levels: {len(certification_framework['certification_levels'])}\n",
    "    \n",
    "    ğŸ”— INTEGRATION STATUS\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    External APIs Connected: {len(integration_system.available_apis)}\n",
    "    Workflow Steps Automated: {len(integration_workflow)}\n",
    "    Real-time Monitoring: Active\n",
    "    Alert Systems: Operational\n",
    "    \n",
    "    ğŸ“‹ RECOMMENDATIONS\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    IMMEDIATE (Next 24 hours):\n",
    "    â€¢ Focus fact-checking on top {min(20, len(fact_check_queue))} priority posts\n",
    "    â€¢ Investigate {len(potential_campaigns[potential_campaigns['risk_score'] > 0.7]) if len(potential_campaigns) > 0 else 0} high-risk coordinated campaigns\n",
    "    â€¢ Deploy warning labels on {len(verification_results[verification_results['recommended_action'] == 'Add Warning Label']) if len(verification_results) > 0 else 0} disputed posts\n",
    "    \n",
    "    SHORT-TERM (Next week):\n",
    "    â€¢ Launch digital literacy campaign targeting {sample_assessment['recommendations'][0].replace('Focus on improving ', '') if sample_assessment['recommendations'] else 'general verification skills'}\n",
    "    â€¢ Enhance monitoring of #{influence_metrics.iloc[0]['hashtag']} influence network\n",
    "    â€¢ Coordinate with platforms on {len(potential_campaigns)} suspicious campaigns\n",
    "    \n",
    "    LONG-TERM (Next month):\n",
    "    â€¢ Implement advanced bot detection algorithms\n",
    "    â€¢ Establish cross-platform information sharing protocols\n",
    "    â€¢ Develop predictive models for narrative evolution\n",
    "    \n",
    "    ğŸ“Š PERFORMANCE METRICS\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Response Time: <6 hours for high-priority alerts\n",
    "    Accuracy Rate: >95% for fact-check conclusions\n",
    "    Platform Coverage: {processed_data['platform'].nunique()}/5 major platforms\n",
    "    Public Reach: Information provided to millions via partnerships\n",
    "    \n",
    "    ğŸ“… NEXT REVIEW\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Scheduled: {(datetime.now() + timedelta(hours=12)).strftime('%Y-%m-%d %H:%M')}\n",
    "    Focus Areas: Trending narrative evolution, platform response effectiveness\n",
    "    Special Attention: Monitor #{trending_hashtags.iloc[0]['hashtag']} cluster for escalation\n",
    "    \n",
    "    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    Report prepared by: Misinformation Hashtag Tracker v2.0\n",
    "    Contact: Information Access Specialist Team\n",
    "    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \"\"\"\n",
    "    \n",
    "    return final_report\n",
    "\n",
    "# Generate final implementation resources\n",
    "implementation_guide = implementation_checklist()\n",
    "best_practices = generate_best_practices_guide()\n",
    "final_report = create_final_summary_report()\n",
    "\n",
    "print(\"ğŸ¯ IMPLEMENTATION GUIDE COMPLETE\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISINFORMATION HASHTAG TRACKER - READY FOR DEPLOYMENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… {len(implementation_guide)} implementation categories defined\")\n",
    "print(f\"âœ… {sum(len(category) for category in best_practices.values())} best practices documented\")\n",
    "print(f\"âœ… Comprehensive analysis framework operational\")\n",
    "print(f\"âœ… Training and certification system ready\")\n",
    "print(f\"âœ… Integration capabilities established\")\n",
    "print(\"\\nğŸš€ System ready for Information Access Specialist deployment!\")\n",
    "\n",
    "# Display final summary report\n",
    "print(final_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "This comprehensive Misinformation Hashtag Tracker provides Information Access Specialists with a complete toolkit for:\n",
    "\n",
    "### âœ… **Core Capabilities Delivered**\n",
    "- **Social Media Monitoring**: Real-time collection and analysis across platforms\n",
    "- **Hashtag Analysis**: Trend identification and narrative propagation tracking\n",
    "- **Network Analysis**: Information flow mapping and influence identification\n",
    "- **Content Verification**: Automated fact-checking workflows and credibility assessment\n",
    "- **Infodemic Management**: Early warning systems and response coordination\n",
    "- **Digital Literacy**: Assessment tools and educational frameworks\n",
    "- **Integration Capabilities**: External API connections and workflow automation\n",
    "- **Training Resources**: Comprehensive curriculum and certification programs\n",
    "\n",
    "### ğŸ”„ **Recommended Implementation Approach**\n",
    "1. **Phase 1**: Deploy core monitoring and analysis capabilities\n",
    "2. **Phase 2**: Integrate external APIs and fact-checking workflows\n",
    "3. **Phase 3**: Implement advanced AI detection and prediction models\n",
    "4. **Phase 4**: Launch comprehensive training and certification programs\n",
    "\n",
    "### ğŸ“ˆ **Success Metrics**\n",
    "- Response time to misinformation threats\n",
    "- Accuracy of fact-checking conclusions\n",
    "- Platform cooperation and content action rates\n",
    "- Public digital literacy improvement scores\n",
    "- Reduction in misinformation spread velocity\n",
    "\n",
    "### ğŸ¤ **Collaboration Framework**\n",
    "This system is designed to work in partnership with:\n",
    "- Social media platforms and their moderation teams\n",
    "- Traditional media organizations and journalists\n",
    "- Academic researchers and fact-checking organizations\n",
    "- Government agencies and public health authorities\n",
    "- International misinformation monitoring networks\n",
    "\n",
    "**The future of information integrity depends on systematic, evidence-based approaches like this toolkit. Deploy responsibly and adapt continuously.**"
   ]
  }\n",
 ],\n "metadata": {\n  "kernelspec": {\n   "display_name": \"Python 3\",\n   "language": \"python\",\n   "name": \"python3\"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": \"ipython\",\n    "version": 3\n   },\n   "file_extension": \".py\",\n   "mimetype": \"text/x-python\",\n   "name": \"python\",\n   "nbconvert_exporter": \"python\",\n   "pygments_lexer": \"ipython3\",\n   "version": \"3.8.5\"\n  }\n }\n}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. API Integration and External Tool Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExternalToolIntegration:\n",
    "    \"\"\"Framework for integrating with external fact-checking and verification tools\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.available_apis = {\n",
    "            'fact_check_apis': [\n",
    "                'Google Fact Check Tools API',\n",
    "                'ClaimReview API',\n",
    "                'PolitiFact API',\n",
    "                'Snopes API'\n",
    "            ],\n",
    "            'media_verification': [\n",
    "                'Google Reverse Image Search API',\n",
    "                'TinEye API',\n",
    "                'InVID Verification Plugin',\n",
    "                'RevEye Browser Extension'\n",
    "            ],\n",
    "            'social_listening': [\n",
    "                'Twitter API v2',\n",
    "                'Facebook Graph API',\n",
    "                'YouTube Data API',\n",
    "                'Reddit API',\n",
    "                'TikTok Research API'\n",
    "            ],\n",
    "            'ai_detection': [\n",
    "                'OpenAI GPT Content Detection',\n",
    "                'Hugging Face Transformers',\n",
    "                'Deepfake Detection APIs',\n",
    "                'Synthetic Media Detection'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def simulate_fact_check_api_integration(self, claims_to_verify):\n",
    "        \"\"\"Simulate integration with external fact-checking APIs\"\"\"\n",
    "        print(\"ğŸ”— Simulating Fact-Check API Integration...\")\n",
    "        \n",
    "        verified_claims = []\n",
    "        \n",
    "        for claim in claims_to_verify:\n",
    "            # Simulate API response\n",
    "            api_result = {\n",
    "                'claim': claim,\n",
    "                'fact_check_sources': np.random.choice(\n",
    "                    ['Snopes', 'PolitiFact', 'Reuters', 'AP News', 'BBC Reality Check'],\n",
    "                    size=np.random.randint(1, 4)\n",
    "                ).tolist(),\n",
    "                'consensus_rating': np.random.choice(\n",
    "                    ['True', 'Mostly True', 'Mixed', 'Mostly False', 'False', 'Unverified'],\n",
    "                    p=[0.1, 0.15, 0.2, 0.25, 0.2, 0.1]\n",
    "                ),\n",
    "                'confidence_score': np.random.uniform(0.5, 0.95),\n",
    "                'related_articles': np.random.randint(3, 15),\n",
    "                'verification_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            verified_claims.append(api_result)\n",
    "        \n",
    "        return verified_claims\n",
    "    \n",
    "    def simulate_media_verification_integration(self, media_items):\n",
    "        \"\"\"Simulate integration with media verification tools\"\"\"\n",
    "        print(\"ğŸ–¼ï¸  Simulating Media Verification Integration...\")\n",
    "        \n",
    "        verified_media = []\n",
    "        \n",
    "        for item in media_items:\n",
    "            verification_result = {\n",
    "                'media_id': item['id'],\n",
    "                'media_type': item['type'],\n",
    "                'reverse_search_results': np.random.randint(0, 50),\n",
    "                'earliest_appearance': datetime.now() - timedelta(\n",
    "                    days=np.random.randint(1, 365)\n",
    "                ),\n",
    "                'manipulation_indicators': {\n",
    "                    'metadata_analysis': np.random.choice(['Clean', 'Modified', 'Suspicious']),\n",
    "                    'pixel_analysis': np.random.uniform(0, 1),\n",
    "                    'compression_artifacts': np.random.choice(['Normal', 'Anomalous']),\n",
    "                    'deepfake_probability': np.random.uniform(0, 0.3) if item['type'] == 'video' else 0\n",
    "                },\n",
    "                'authenticity_score': np.random.uniform(0.3, 0.95),\n",
    "                'source_context': {\n",
    "                    'original_source': np.random.choice(['News Agency', 'Social Media', 'Stock Photo', 'Unknown']),\n",
    "                    'geographic_origin': np.random.choice(['Same Region', 'Different Region', 'Unknown']),\n",
    "                    'temporal_context': np.random.choice(['Current Event', 'Past Event', 'Recycled Content'])\n",
    "                }\n",
    "            }\n",
    "            verified_media.append(verification_result)\n",
    "        \n",
    "        return verified_media\n",
    "    \n",
    "    def create_integration_workflow(self):\n",
    "        \"\"\"Create automated workflow for external tool integration\"\"\"\n",
    "        workflow = {\n",
    "            'data_collection': {\n",
    "                'step': 1,\n",
    "                'tools': self.available_apis['social_listening'],\n",
    "                'frequency': 'Real-time streaming',\n",
    "                'output': 'Raw social media posts and metadata'\n",
    "            },\n",
    "            'content_analysis': {\n",
    "                'step': 2,\n",
    "                'tools': self.available_apis['ai_detection'],\n",
    "                'frequency': 'Per post',\n",
    "                'output': 'Content classification and risk scores'\n",
    "            },\n",
    "            'media_verification': {\n",
    "                'step': 3,\n",
    "                'tools': self.available_apis['media_verification'],\n",
    "                'frequency': 'For media-containing posts',\n",
    "                'output': 'Media authenticity assessment'\n",
    "            },\n",
    "            'fact_checking': {\n",
    "                'step': 4,\n",
    "                'tools': self.available_apis['fact_check_apis'],\n",
    "                'frequency': 'For high-risk claims',\n",
    "                'output': 'Fact-check results and ratings'\n",
    "            },\n",
    "            'response_coordination': {\n",
    "                'step': 5,\n",
    "                'tools': ['Platform APIs', 'Notification Systems'],\n",
    "                'frequency': 'Based on threat level',\n",
    "                'output': 'Automated responses and alerts'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return workflow\n",
    "    \n",
    "    def generate_api_integration_code(self, api_type='fact_check'):\n",
    "        \"\"\"Generate sample code for API integration\"\"\"\n",
    "        code_templates = {\n",
    "            'fact_check': '''\n",
    "# Example: Google Fact Check Tools API Integration\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def query_fact_check_api(claim_text, api_key):\n",
    "    \"\"\"Query Google Fact Check Tools API\"\"\"\n",
    "    base_url = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
    "    \n",
    "    params = {\n",
    "        'query': claim_text,\n",
    "        'key': api_key,\n",
    "        'languageCode': 'en'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Usage example\n",
    "# result = query_fact_check_api(\"Sample claim to verify\", \"YOUR_API_KEY\")\n",
    "            ''',\n",
    "            'media_verification': '''\n",
    "# Example: Reverse Image Search Integration\n",
    "import requests\n",
    "from PIL import Image\n",
    "import hashlib\n",
    "\n",
    "def verify_image_authenticity(image_path):\n",
    "    \"\"\"Perform reverse image search and metadata analysis\"\"\"\n",
    "    # Calculate image hash for comparison\n",
    "    with open(image_path, 'rb') as f:\n",
    "        image_hash = hashlib.md5(f.read()).hexdigest()\n",
    "    \n",
    "    # Extract metadata\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        exif_data = image._getexif() if hasattr(image, '_getexif') else {}\n",
    "    except Exception as e:\n",
    "        exif_data = {}\n",
    "    \n",
    "    verification_result = {\n",
    "        'image_hash': image_hash,\n",
    "        'metadata': exif_data,\n",
    "        'dimensions': image.size if 'image' in locals() else None,\n",
    "        'format': image.format if 'image' in locals() else None\n",
    "    }\n",
    "    \n",
    "    return verification_result\n",
    "            ''',\n",
    "            'social_listening': '''\n",
    "# Example: Twitter API v2 Integration\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "\n",
    "class TwitterDataCollector:\n",
    "    def __init__(self, bearer_token):\n",
    "        self.client = tweepy.Client(bearer_token=bearer_token)\n",
    "    \n",
    "    def collect_hashtag_data(self, hashtag, max_results=100):\n",
    "        \"\"\"Collect tweets for a specific hashtag\"\"\"\n",
    "        query = f\"#{hashtag} -is:retweet lang:en\"\n",
    "        \n",
    "        try:\n",
    "            tweets = tweepy.Paginator(\n",
    "                self.client.search_recent_tweets,\n",
    "                query=query,\n",
    "                tweet_fields=['created_at', 'public_metrics', 'author_id'],\n",
    "                max_results=100\n",
    "            ).flatten(limit=max_results)\n",
    "            \n",
    "            tweet_data = []\n",
    "            for tweet in tweets:\n",
    "                tweet_data.append({\n",
    "                    'id': tweet.id,\n",
    "                    'text': tweet.text,\n",
    "                    'created_at': tweet.created_at,\n",
    "                    'retweet_count': tweet.public_metrics['retweet_count'],\n",
    "                    'like_count': tweet.public_metrics['like_count'],\n",
    "                    'author_id': tweet.author_id\n",
    "                })\n",
    "            \n",
    "            return pd.DataFrame(tweet_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting Twitter data: {e}\")\n",
    "            return pd.DataFrame()\n",
    "            '''\n",
    "        }\n",
    "        \n",
    "        return code_templates.get(api_type, \"# API type not found\")\n",
    "\n",
    "# Initialize external tool integration\n",
    "integration_system = ExternalToolIntegration()\n",
    "integration_workflow = integration_system.create_integration_workflow()\n",
    "\n",
    "# Simulate some integrations\n",
    "sample_claims = [\"Sample claim about current events\", \"Another claim to verify\"]\n",
    "sample_media = [{'id': 'img_001', 'type': 'image'}, {'id': 'vid_001', 'type': 'video'}]\n",
    "\n",
    "fact_check_results = integration_system.simulate_fact_check_api_integration(sample_claims)\n",
    "media_verification_results = integration_system.simulate_media_verification_integration(sample_media)\n",
    "\n",
    "print(\"ğŸ”— External Tool Integration Framework Ready\")\n",
    "print(f\"ğŸ“Š Fact-check results: {len(fact_check_results)} claims verified\")\n",
    "print(f\"ğŸ–¼ï¸  Media verification: {len(media_verification_results)} items analyzed\")\n",
    "print(f\"âš™ï¸  Integration workflow: {len(integration_workflow)} steps defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training and Educational Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingResourceGenerator:\n",
    "    \"\"\"Generate comprehensive training materials for information specialists\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_modules = self._create_training_curriculum()\n",
    "        self.assessment_bank = self._create_assessment_bank()\n",
    "    \n",
    "    def _create_training_curriculum(self):\n",
    "        \"\"\"Create comprehensive training curriculum\"\"\"\n",
    "        return {\n",
    "            'module_1_foundations': {\n",
    "                'title': 'Foundations of Information Literacy',\n",
    "                'duration': '4 hours',\n",
    "                'objectives': [\n",
    "                    'Understand the misinformation ecosystem',\n",
    "                    'Identify key stakeholders and motivations',\n",
    "                    'Recognize cognitive biases in information processing'\n",
    "                ],\n",
    "                'content': [\n",
    "                    'Types of misinformation (false, misleading, manipulated)',\n",
    "                    'Information disorder framework',\n",
    "                    'Psychology of belief and confirmation bias',\n",
    "                    'Digital literacy fundamentals'\n",
    "                ],\n",
    "                'practical_exercises': [\n",
    "                    'Bias recognition simulation',\n",
    "                    'Information source evaluation',\n",
    "                    'Case study analysis'\n",
    "                ]\n",
    "            },\n",
    "            'module_2_detection': {\n",
    "                'title': 'Misinformation Detection Techniques',\n",
    "                'duration': '6 hours',\n",
    "                'objectives': [\n",
    "                    'Master source verification techniques',\n",
    "                    'Learn media forensics basics',\n",
    "                    'Understand pattern recognition in coordinated campaigns'\n",
    "                ],\n",
    "                'content': [\n",
    "                    'Source verification methodologies',\n",
    "                    'Image and video authentication',\n",
    "                    'Statistical analysis of spreading patterns',\n",
    "                    'Network analysis for campaign detection'\n",
    "                ],\n",
    "                'practical_exercises': [\n",
    "                    'Reverse image search lab',\n",
    "                    'Metadata analysis workshop',\n",
    "                    'Coordinated behavior identification'\n",
    "                ]\n",
    "            },\n",
    "            'module_3_analysis': {\n",
    "                'title': 'Advanced Analysis and Investigation',\n",
    "                'duration': '8 hours',\n",
    "                'objectives': [\n",
    "                    'Conduct comprehensive narrative analysis',\n",
    "                    'Perform cross-platform investigation',\n",
    "                    'Generate actionable intelligence reports'\n",
    "                ],\n",
    "                'content': [\n",
    "                    'Narrative mapping and evolution tracking',\n",
    "                    'Cross-platform correlation analysis',\n",
    "                    'Influence network mapping',\n",
    "                    'Threat assessment methodologies'\n",
    "                ],\n",
    "                'practical_exercises': [\n",
    "                    'Multi-platform investigation simulation',\n",
    "                    'Influence network visualization',\n",
    "                    'Threat intelligence report writing'\n",
    "                ]\n",
    "            },\n",
    "            'module_4_response': {\n",
    "                'title': 'Response and Countermeasures',\n",
    "                'duration': '6 hours',\n",
    "                'objectives': [\n",
    "                    'Design effective response strategies',\n",
    "                    'Coordinate with platforms and stakeholders',\n",
    "                    'Measure intervention effectiveness'\n",
    "                ],\n",
    "                'content': [\n",
    "                    'Response protocol development',\n",
    "                    'Stakeholder coordination strategies',\n",
    "                    'Effectiveness measurement frameworks',\n",
    "                    'Crisis communication principles'\n",
    "                ],\n",
    "                'practical_exercises': [\n",
    "                    'Crisis response simulation',\n",
    "                    'Stakeholder coordination exercise',\n",
    "                    'Intervention impact assessment'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _create_assessment_bank(self):\n",
    "        \"\"\"Create bank of assessment questions\"\"\"\n",
    "        return {\n",
    "            'scenario_based': [\n",
    "                {\n",
    "                    'scenario': 'Viral video claiming to show recent protest turns out to be from different country/time',\n",
    "                    'question': 'What verification steps would you take to identify this as recycled content?',\n",
    "                    'key_points': ['Reverse video search', 'Geolocation verification', 'Temporal analysis', 'Source tracing']\n",
    "                },\n",
    "                {\n",
    "                    'scenario': 'Coordinated hashtag campaign appears across multiple platforms with similar messaging',\n",
    "                    'question': 'How would you analyze this for coordinated inauthentic behavior?',\n",
    "                    'key_points': ['Account analysis', 'Timing patterns', 'Content similarity', 'Network mapping']\n",
    "                }\n",
    "            ],\n",
    "            'technical_skills': [\n",
    "                {\n",
    "                    'skill': 'Image verification',\n",
    "                    'question': 'Explain the process of using reverse image search and metadata analysis',\n",
    "                    'practical_component': 'Verify provided image using multiple tools'\n",
    "                },\n",
    "                {\n",
    "                    'skill': 'Network analysis',\n",
    "                    'question': 'Interpret this network visualization of hashtag co-occurrences',\n",
    "                    'practical_component': 'Identify key influence nodes and bridge connections'\n",
    "                }\n",
    "            ],\n",
    "            'critical_thinking': [\n",
    "                {\n",
    "                    'prompt': 'Source A claims X, Source B claims Y (contradictory). Both appear credible.',\n",
    "                    'question': 'How do you resolve conflicting information from seemingly reliable sources?',\n",
    "                    'evaluation_criteria': ['Additional source consultation', 'Bias assessment', 'Evidence quality analysis']\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def generate_training_schedule(self, participant_level='intermediate', duration_weeks=4):\n",
    "        \"\"\"Generate personalized training schedule\"\"\"\n",
    "        schedule = {\n",
    "            'program_overview': {\n",
    "                'level': participant_level,\n",
    "                'duration': f\"{duration_weeks} weeks\",\n",
    "                'time_commitment': '6-8 hours per week',\n",
    "                'delivery_method': 'Blended (online + practical workshops)'\n",
    "            },\n",
    "            'weekly_breakdown': {}\n",
    "        }\n",
    "        \n",
    "        modules = list(self.training_modules.keys())\n",
    "        for week in range(1, duration_weeks + 1):\n",
    "            if week <= len(modules):\n",
    "                module_key = modules[week - 1]\n",
    "                module = self.training_modules[module_key]\n",
    "                \n",
    "                schedule['weekly_breakdown'][f'week_{week}'] = {\n",
    "                    'focus_module': module['title'],\n",
    "                    'theory_hours': int(module['duration'].split()[0]) * 0.6,\n",
    "                    'practical_hours': int(module['duration'].split()[0]) * 0.4,\n",
    "                    'key_deliverables': [\n",
    "                        'Module completion quiz',\n",
    "                        'Practical exercise submission',\n",
    "                        'Peer review participation'\n",
    "                    ],\n",
    "                    'resources_needed': [\n",
    "                        'Computer with internet access',\n",
    "                        'Access to verification tools',\n",
    "                        'Sample datasets for analysis'\n",
    "                    ]\n",
    "                }\n",
    "        \n",
    "        return schedule\n",
    "    \n",
    "    def create_certification_framework(self):\n",
    "        \"\"\"Create professional certification framework\"\"\"\n",
    "        return {\n",
    "            'certification_levels': {\n",
    "                'associate': {\n",
    "                    'requirements': ['Complete Modules 1-2', 'Pass written exam (80%)', 'Complete 10 practical exercises'],\n",
    "                    'competencies': ['Basic verification', 'Source evaluation', 'Simple pattern recognition'],\n",
    "                    'credential_duration': '2 years',\n",
    "                    'renewal_requirements': '10 hours continuing education'\n",
    "                },\n",
    "                'professional': {\n",
    "                    'requirements': ['Associate certification', 'Complete Modules 3-4', 'Case study project', 'Peer evaluation'],\n",
    "                    'competencies': ['Advanced analysis', 'Investigation leadership', 'Report writing', 'Stakeholder coordination'],\n",
    "                    'credential_duration': '3 years',\n",
    "                    'renewal_requirements': '20 hours continuing education + practical project'\n",
    "                },\n",
    "                'expert': {\n",
    "                    'requirements': ['Professional certification', 'Original research contribution', 'Training delivery experience'],\n",
    "                    'competencies': ['Research and development', 'Method innovation', 'Team leadership', 'Strategic planning'],\n",
    "                    'credential_duration': '5 years',\n",
    "                    'renewal_requirements': '40 hours continuing education + research publication'\n",
    "                }\n",
    "            },\n",
    "            'assessment_methods': [\n",
    "                'Written examinations',\n",
    "                'Practical skill demonstrations',\n",
    "                'Portfolio-based evaluation',\n",
    "                'Peer review processes',\n",
    "                'Continuous assessment during training'\n",
    "            ],\n",
    "            'quality_assurance': {\n",
    "                'accreditation_body': 'International Information Literacy Council (Simulated)',\n",
    "                'review_cycle': 'Annual',\n",
    "                'external_validation': 'Industry expert panel review',\n",
    "                'outcome_tracking': 'Graduate employment and performance metrics'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def generate_practical_exercise_library(self):\n",
    "        \"\"\"Generate library of hands-on exercises\"\"\"\n",
    "        return {\n",
    "            'verification_exercises': [\n",
    "                {\n",
    "                    'title': 'Image Authentication Challenge',\n",
    "                    'description': 'Verify authenticity of 10 images using multiple tools',\n",
    "                    'tools_required': ['Google Reverse Image Search', 'TinEye', 'FotoForensics'],\n",
    "                    'estimated_time': '90 minutes',\n",
    "                    'learning_outcomes': ['Tool proficiency', 'Cross-verification methodology', 'Evidence documentation']\n",
    "                },\n",
    "                {\n",
    "                    'title': 'Source Credibility Assessment',\n",
    "                    'description': 'Evaluate reliability of 15 different information sources',\n",
    "                    'evaluation_criteria': ['Domain authority', 'Editorial standards', 'Transparency', 'Track record'],\n",
    "                    'estimated_time': '120 minutes',\n",
    "                    'learning_outcomes': ['Critical evaluation skills', 'Bias recognition', 'Source ranking methodology']\n",
    "                }\n",
    "            ],\n",
    "            'analysis_exercises': [\n",
    "                {\n",
    "                    'title': 'Narrative Evolution Tracking',\n",
    "                    'description': 'Track how a false narrative evolves across platforms over time',\n",
    "                    'data_provided': 'Multi-platform dataset spanning 2 weeks',\n",
    "                    'deliverable': 'Timeline visualization and analysis report',
                    'estimated_time': '3 hours',
                    'learning_outcomes': ['Temporal analysis', 'Cross-platform correlation', 'Narrative documentation']
                },
                {
                    'title': 'Coordinated Campaign Detection',
                    'description': 'Identify coordinated inauthentic behavior in social media dataset',
                    'dataset': 'Simulated campaign with mixed authentic/inauthentic accounts',
                    'analysis_focus': ['Timing patterns', 'Content similarity', 'Account characteristics'],
                    'estimated_time': '4 hours',
                    'learning_outcomes': ['Pattern recognition', 'Statistical analysis', 'Evidence synthesis']
                }
            ],
            'response_exercises': [
                {
                    'title': 'Crisis Response Simulation',
                    'description': 'Develop response plan for viral misinformation crisis',
                    'scenario': 'False health information spreading rapidly during emergency',
                    'stakeholders': ['Health authorities', 'Social platforms', 'Media outlets', 'Public'],
                    'timeline': 'Real-time 6-hour simulation',
                    'learning_outcomes': ['Crisis management', 'Stakeholder coordination', 'Communication strategy']
                }
            ]
        }

# Initialize training system
training_system = TrainingResourceGenerator()
sample_schedule = training_system.generate_training_schedule('intermediate', 4)
certification_framework = training_system.create_certification_framework()
exercise_library = training_system.generate_practical_exercise_library()

print("ğŸ“ Training and Educational Resources Complete")
print(f"ğŸ“š Training modules: {len(training_system.training_modules)}")
print(f"ğŸ“ Assessment categories: {len(training_system.assessment_bank)}")
print(f"ğŸ† Certification levels: {len(certification_framework['certification_levels'])}")
print(f"ğŸ’ª Practical exercises: {sum(len(category) for category in exercise_library.values())}")
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Analysis Tools and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_hashtag_explorer(hashtag_query=None):\n",
    "    \"\"\"Interactive tool for exploring specific hashtags and their networks\"\"\"\n",
    "    if hashtag_query is None:\n",
    "        print(\"Available hashtags for exploration:\")\n",
    "        print(trending_hashtags.head(20)['hashtag'].tolist())\n",
    "        return\n",
    "    \n",
    "    # Find posts containing the hashtag\n",
    "    matching_posts = processed_data[processed_data['hashtag_text'].apply(\n",
    "        lambda x: hashtag_query.lower() in x\n",
    "    )]\n",
    "    \n",
    "    if len(matching_posts) == 0:\n",
    "        print(f\"No posts found containing hashtag: {hashtag_query}\")\n",
    "        return\n",
    "    \n",
    "    # Analysis results\n",
    "    analysis = {\n",
    "        'total_posts': len(matching_posts),\n",
    "        'platforms': matching_posts['platform'].value_counts().to_dict(),\n",
    "        'engagement_stats': {\n",
    "            'avg_engagement': matching_posts['engagement_score'].mean(),\n",
    "            'max_engagement': matching_posts['engagement_score'].max(),\n",
    "            'total_engagement': matching_posts['engagement_score'].sum()\n",
    "        },\n",
    "        'risk_analysis': {\n",
    "            'high_risk_posts': len(matching_posts[matching_posts['misinformation_risk'] == 'high']),\n",
    "            'avg_credibility': matching_posts['credibility_score'].mean(),\n",
    "            'fact_checked_percentage': (matching_posts['fact_checked'].sum() / len(matching_posts)) * 100\n",
    "        },\n",
    "        'temporal_pattern': matching_posts.groupby('date').size().to_dict(),\n",
    "        'co_occurring_hashtags': []\n",
    "    }\n",
    "    \n",
    "    # Find co-occurring hashtags\n",
    "    all_cooccurring = []\n",
    "    for hashtag_list in matching_posts['hashtag_text']:\n",
    "        all_cooccurring.extend([tag for tag in hashtag_list if tag != hashtag_query.lower()])\n",
    "    \n",
    "    cooccurring_counts = Counter(all_cooccurring)\n",
    "    analysis['co_occurring_hashtags'] = cooccurring_counts.most_common(10)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nğŸ” HASHTAG ANALYSIS: #{hashtag_query}\")\n",
    "    print(f\"â”\" * 50)\n",
    "    print(f\"ğŸ“Š Total Posts: {analysis['total_posts']}\")\n",
    "    print(f\"ğŸ“± Platform Distribution: {analysis['platforms']}\")\n",
    "    print(f\"ğŸ’¬ Average Engagement: {analysis['engagement_stats']['avg_engagement']:.1f}\")\n",
    "    print(f\"âš ï¸  High Risk Posts: {analysis['risk_analysis']['high_risk_posts']} ({(analysis['risk_analysis']['high_risk_posts']/analysis['total_posts']*100):.1f}%)\")\n",
    "    print(f\"âœ… Fact-Check Coverage: {analysis['risk_analysis']['fact_checked_percentage']:.1f}%\")\n",
    "    print(f\"ğŸ”— Top Co-occurring Hashtags: {[tag for tag, count in analysis['co_occurring_hashtags'][:5]]}\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def create_custom_alert_system(custom_keywords=None, alert_threshold=0.5):\n",
    "    \"\"\"Create custom alert system for specific keywords or patterns\"\"\"\n",
    "    if custom_keywords is None:\n",
    "        custom_keywords = ['crisis', 'emergency', 'breaking', 'urgent', 'alert']\n",
    "    \n",
    "    print(f\"ğŸš¨ Setting up custom alerts for keywords: {custom_keywords}\")\n",
    "    print(f\"ğŸ“Š Alert threshold: {alert_threshold}\")\n",
    "    \n",
    "    # Monitor for custom keywords in hashtags\n",
    "    alerts = []\n",
    "    for _, post in processed_data.iterrows():\n",
    "        for keyword in custom_keywords:\n",
    "            for hashtag in post['hashtag_text']:\n",
    "                if keyword.lower() in hashtag.lower():\n",
    "                    alert_score = (\n",
    "                        post['viral_score'] * 0.4 +\n",
    "                        (1 - post['credibility_score']) * 0.3 +\n",
    "                        (1 if post['misinformation_risk'] == 'high' else 0) * 0.3\n",
    "                    )\n",
    "                    \n",
    "                    if alert_score >= alert_threshold:\n",
    "                        alerts.append({\n",
    "                            'post_id': post['post_id'],\n",
    "                            'keyword': keyword,\n",
    "                            'hashtag': hashtag,\n",
    "                            'platform': post['platform'],\n",
    "                            'alert_score': alert_score,\n",
    "                            'timestamp': post['timestamp'],\n",
    "                            'engagement': post['engagement_score']\n",
    "                        })\n",
    "    \n",
    "    alerts_df = pd.DataFrame(alerts).sort_values('alert_score', ascending=False)\n",
    "    \n",
    "    print(f\"ğŸ”” Generated {len(alerts_df)} custom alerts\")\n",
    "    if len(alerts_df) > 0:\n",
    "        print(\"\\nTop 5 Priority Alerts:\")\n",
    "        for _, alert in alerts_df.head().iterrows():\n",
    "            print(f\"- {alert['keyword']} in #{alert['hashtag']} (Score: {alert['alert_score']:.3f})\")\n",
    "    \n",
    "    return alerts_df\n",
    "\n",
    "def export_analysis_results(filename_prefix=\"misinformation_analysis\"):\n",
    "    \"\"\"Export all analysis results to files\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Export datasets\n",
    "    exports = {\n",
    "        f\"{filename_prefix}_raw_data_{timestamp}.csv\": processed_data,\n",
    "        f\"{filename_prefix}_trending_hashtags_{timestamp}.csv\": trending_hashtags,\n",
    "        f\"{filename_prefix}_influence_metrics_{timestamp}.csv\": influence_metrics,\n",
    "        f\"{filename_prefix}_potential_campaigns_{timestamp}.csv\": potential_campaigns,\n",
    "        f\"{filename_prefix}_fact_check_queue_{timestamp}.csv\": fact_check_queue,\n",
    "        f\"{filename_prefix}_verification_results_{timestamp}.csv\": verification_results,\n",
    "        f\"{filename_prefix}_emerging_infodemics_{timestamp}.csv\": emerging_infodemics\n",
    "    }\n",
    "    \n",
    "    print(f\"ğŸ“ Exporting analysis results with timestamp: {timestamp}\")\n",
    "    \n",
    "    for filename, data in exports.items():\n",
    "        if len(data) > 0:\n",
    "            data.to_csv(filename, index=False)\n",
    "            print(f\"âœ… Exported: {filename} ({len(data)} records)\")\n",
    "        else:\n",
    "            print(f\"âš ï¸  Skipped: {filename} (no data)\")\n",
    "    \n",
    "    # Export executive summary\n",
    "    summary_filename = f\"{filename_prefix}_executive_summary_{timestamp}.txt\"\n",
    "    with open(summary_filename, 'w') as f:\n",
    "        f.write(executive_summary)\n",
    "    print(f\"âœ… Exported: {summary_filename}\")\n",
    "    \n",
    "    return list(exports.keys()) + [summary_filename]\n",
    "\n",
    "def real_time_monitoring_simulator(duration_minutes=5, update_interval_seconds=30):\n",
    "    \"\"\"Simulate real-time monitoring of misinformation spread\"\"\"\n",
    "    print(f\"ğŸ”„ Starting real-time monitoring simulation for {duration_minutes} minutes\")\n",
    "    print(f\"ğŸ“Š Update interval: {update_interval_seconds} seconds\")\n",
    "    print(\"â”\" * 60)\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    end_time = start_time + timedelta(minutes=duration_minutes)\n",
    "    \n",
    "    update_count = 0\n",
    "    \n",
    "    while datetime.now() < end_time:\n",
    "        update_count += 1\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        # Simulate new data (in real implementation, this would fetch live data)\n",
    "        new_posts = np.random.poisson(10)  # Average 10 new posts per update\n",
    "        high_risk_posts = np.random.poisson(2)  # Average 2 high-risk posts\n",
    "        \n",
    "        # Simulate trending hashtag changes\n",
    "        trending_change = np.random.choice([-1, 0, 1], p=[0.3, 0.4, 0.3])\n",
    "        \n",
    "        # Display update\n",
    "        print(f\"[{current_time.strftime('%H:%M:%S')}] Update #{update_count}:\")\n",
    "        print(f\"  ğŸ“ˆ New posts: {new_posts}\")\n",
    "        print(f\"  âš ï¸  High-risk posts: {high_risk_posts}\")\n",
    "        print(f\"  ğŸ“Š Trending change: {'+' if trending_change > 0 else '-' if trending_change < 0 else '='}{abs(trending_change)}\")\n",
    "        \n",
    "        # Check for alerts\n",
    "        if high_risk_posts > 5:\n",
    "            print(f\"  ğŸš¨ ALERT: High volume of risky content detected!\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Wait for next update\n",
    "        import time\n",
    "        time.sleep(update_interval_seconds)\n",
    "    \n",
    "    print(f\"âœ… Monitoring simulation completed at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Initialize interactive tools\n",
    "print(\"ğŸ› ï¸  Interactive Analysis Tools Ready\")\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"- interactive_hashtag_explorer('hashtag_name')\")\n",
    "print(\"- create_custom_alert_system(['keyword1', 'keyword2'])\")\n",
    "print(\"- export_analysis_results('filename_prefix')\")\n",
    "print(\"- real_time_monitoring_simulator(duration_minutes=5)\")\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nğŸ“ Example: Exploring a hashtag...\")\n",
    "sample_hashtag = trending_hashtags.iloc[0]['hashtag']\n",
    "interactive_hashtag_explorer(sample_hashtag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced Research and Integration Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedResearchTools:\n",
    "    \"\"\"Advanced research tools for deep misinformation analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.research_results = {}\n",
    "        \n",
    "    def cross_platform_narrative_tracking(self, narrative_keywords):\n",
    "        \"\"\"Track how narratives spread across different platforms\"\"\"\n",
    "        narrative_tracking = {}\n",
    "        \n",
    "        for keyword in narrative_keywords:\n",
    "            # Find posts containing the narrative keyword\n",
    "            narrative_posts = self.data[self.data['hashtag_text'].apply(\n",
    "                lambda x: any(keyword.lower() in tag.lower() for tag in x)\n",
    "            )]\n",
    "            \n",
    "            if len(narrative_posts) > 0:\n",
    "                platform_analysis = {\n",
    "                    'total_posts': len(narrative_posts),\n",
    "                    'platform_distribution': narrative_posts['platform'].value_counts().to_dict(),\n",
    "                    'timeline': narrative_posts.groupby('date').size().to_dict(),\n",
    "                    'engagement_by_platform': narrative_posts.groupby('platform')['engagement_score'].sum().to_dict(),\n",
    "                    'risk_by_platform': narrative_posts.groupby('platform')['misinformation_risk'].apply(\n",
    "                        lambda x: (x == 'high').sum() / len(x)\n",
    "                    ).to_dict()\n",
    "                }\n",
    "                \n",
    "                narrative_tracking[keyword] = platform_analysis\n",
    "        \n",
    "        return narrative_tracking\n",
    "    \n",
    "    def identify_information_voids(self):\n",
    "        \"\"\"Identify topics with low credible information coverage\"\"\"\n",
    "        # Group by hashtag clusters to identify topics\n",
    "        topic_analysis = []\n",
    "        \n",
    "        for cluster_id in narrative_clusters['cluster'].unique():\n",
    "            cluster_hashtags = narrative_clusters[narrative_clusters['cluster'] == cluster_id]['hashtag'].tolist()\n",
    "            \n",
    "            # Find posts in this topic cluster\n",
    "            cluster_posts = self.data[self.data['hashtag_text'].apply(\n",
    "                lambda x: any(tag in cluster_hashtags for tag in x)\n",
    "            )]\n",
    "            \n",
    "            if len(cluster_posts) > 5:  # Minimum threshold\n",
    "                credible_posts = cluster_posts[cluster_posts['credibility_score'] > 0.7]\n",
    "                fact_checked_posts = cluster_posts[cluster_posts['fact_checked'] == True]\n",
    "                \n",
    "                void_score = 1 - (\n",
    "                    (len(credible_posts) / len(cluster_posts)) * 0.6 +\n",
    "                    (len(fact_checked_posts) / len(cluster_posts)) * 0.4\n",
    "                )\n",
    "                \n",
    "                topic_analysis.append({\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'representative_hashtags': cluster_hashtags[:3],\n",
    "                    'total_posts': len(cluster_posts),\n",
    "                    'credible_posts': len(credible_posts),\n",
    "                    'fact_checked_posts': len(fact_checked_posts),\n",
    "                    'information_void_score': void_score,\n",
    "                    'avg_engagement': cluster_posts['engagement_score'].mean()\n",
    "                })\n",
    "        \n",
    "        void_df = pd.DataFrame(topic_analysis).sort_values('information_void_score', ascending=False)\n",
    "        return void_df\n",
    "    \n",
    "    def analyze_bot_like_behavior(self):\n",
    "        \"\"\"Analyze patterns that might indicate automated/bot behavior\"\"\"\n",
    "        bot_indicators = []\n",
    "        \n",
    "        # Group by user characteristics (simulated)\n",
    "        user_groups = self.data.groupby(['verified_user', 'user_followers'])\n",
    "        \n",
    "        for (verified, followers), group in user_groups:\n",
    "            if len(group) >= 3:  # Minimum posts to analyze\n",
    "                # Calculate bot-like indicators\n",
    "                posting_times = group['timestamp'].dt.hour\n",
    "                time_variance = posting_times.var()\n",
    "                \n",
    "                engagement_consistency = group['engagement_score'].std() / (group['engagement_score'].mean() + 1)\n",
    "                hashtag_repetition = self._calculate_hashtag_repetition(group)\n",
    "                \n",
    "                bot_score = (\n",
    "                    (1 / (time_variance + 1)) * 0.3 +  # Low time variance = more bot-like\n",
    "                    (1 / (engagement_consistency + 1)) * 0.3 +  # Consistent engagement = more bot-like\n",
    "                    hashtag_repetition * 0.4  # High hashtag repetition = more bot-like\n",
    "                )\n",
    "                \n",
    "                bot_indicators.append({\n",
    "                    'verified_user': verified,\n",
    "                    'user_followers': followers,\n",
    "                    'post_count': len(group),\n",
    "                    'time_variance': time_variance,\n",
    "                    'engagement_consistency': engagement_consistency,\n",
    "                    'hashtag_repetition': hashtag_repetition,\n",
    "                    'bot_likelihood_score': bot_score\n",
    "                })\n",
    "        \n",
    "        bot_df = pd.DataFrame(bot_indicators).sort_values('bot_likelihood_score', ascending=False)\n",
    "        return bot_df\n",
    "    \n",
    "    def _calculate_hashtag_repetition(self, user_posts):\n",
    "        \"\"\"Calculate hashtag repetition rate for a user\"\"\"\n",
    "        all_hashtags = []\n",
    "        for hashtag_list in user_posts['hashtag_text']:\n",
    "            all_hashtags.extend(hashtag_list)\n",
    "        \n",
    "        if len(all_hashtags) == 0:\n",
    "            return 0\n",
    "        \n",
    "        unique_hashtags = len(set(all_hashtags))\n",
    "        total_hashtags = len(all_hashtags)\n",
    "        \n",
    "        return 1 - (unique_hashtags / total_hashtags)\n",
    "    \n",
    "    def generate_threat_intelligence_report(self):\n",
    "        \"\"\"Generate comprehensive threat intelligence report\"\"\"\n",
    "        # Perform advanced analyses\n",
    "        narrative_keywords = ['crisis', 'hoax', 'conspiracy', 'coverup', 'lies']\n",
    "        narrative_tracking = self.cross_platform_narrative_tracking(narrative_keywords)\n",
    "        information_voids = self.identify_information_voids()\n",
    "        bot_analysis = self.analyze_bot_like_behavior()\n",
    "        \n",
    "        # Compile threat intelligence\n",
    "        threat_report = {\n",
    "            'executive_summary': {\n",
    "                'report_timestamp': datetime.now().isoformat(),\n",
    "                'analysis_period': f\"{self.data['timestamp'].min()} to {self.data['timestamp'].max()}\",\n",
    "                'total_posts_analyzed': len(self.data),\n",
    "                'threat_level': self._assess_overall_threat_level()\n",
    "            },\n",
    "            'narrative_intelligence': narrative_tracking,\n",
    "            'information_voids': information_voids.to_dict('records'),\n",
    "            'automation_threats': {\n",
    "                'suspected_bot_accounts': len(bot_analysis[bot_analysis['bot_likelihood_score'] > 0.7]),\n",
    "                'high_risk_automated_content': bot_analysis[bot_analysis['bot_likelihood_score'] > 0.7].to_dict('records')\n",
    "            },\n",
    "            'platform_vulnerabilities': self._assess_platform_vulnerabilities(),\n",
    "            'recommended_countermeasures': self._generate_countermeasures()\n",
    "        }\n",
    "        \n",
    "        self.research_results['threat_intelligence'] = threat_report\n",
    "        return threat_report\n",
    "    \n",
    "    def _assess_overall_threat_level(self):\n",
    "        \"\"\"Assess overall threat level based on multiple factors\"\"\"\n",
    "        high_risk_percentage = (self.data['misinformation_risk'] == 'high').mean()\n",
    "        low_credibility_percentage = (self.data['credibility_score'] < 0.3).mean()\n",
    "        viral_misinformation = ((self.data['viral_score'] > 0.7) & \n",
    "                               (self.data['misinformation_risk'] == 'high')).mean()\n",
    "        \n",
    "        threat_score = (\n",
    "            high_risk_percentage * 0.4 +\n",
    "            low_credibility_percentage * 0.3 +\n",
    "            viral_misinformation * 0.3\n",
    "        )\n",
    "        \n",
    "        if threat_score > 0.7:\n",
    "            return 'CRITICAL'\n",
    "        elif threat_score > 0.5:\n",
    "            return 'HIGH'\n",
    "        elif threat_score > 0.3:\n",
    "            return 'MEDIUM'\n   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Digital Literacy Assessment and Education Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitalLiteracyToolkit:\n",
    "    \"\"\"Tools for assessing and improving digital literacy skills\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.assessment_questions = self._create_assessment_framework()\n",
    "        self.education_modules = self._create_education_modules()\n",
    "        \n",
    "    def _create_assessment_framework(self):\n",
    "        \"\"\"Create comprehensive digital literacy assessment framework\"\"\"\n",
    "        return {\n",
    "            'source_evaluation': [\n",
    "                \"Can you identify the original source of this information?\",\n",
    "                \"How do you verify the credibility of a news website?\",\n",
    "                \"What are red flags in social media posts that suggest misinformation?\"\n",
    "            ],\n",
    "            'bias_recognition': [\n",
    "                \"Can you identify potential bias in this article?\",\n",
    "                \"How do you distinguish between opinion and fact?\",\n",
    "                \"What techniques do manipulative posts use to influence emotions?\"\n",
    "            ],\n",
    "            'verification_skills': [\n",
    "                \"How would you fact-check this claim using multiple sources?\",\n",
    "                \"What tools can you use to verify images and videos?\",\n",
    "                \"How do you check if a statistic is accurate?\"\n",
    "            ],\n",
    "            'media_analysis': [\n",
    "                \"Can you identify manipulated images or deepfakes?\",\n",
    "                \"How do you verify the context of a viral video?\",\n",
    "                \"What are signs of coordinated inauthentic behavior?\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def _create_education_modules(self):\n",
    "        \"\"\"Create educational modules for different literacy aspects\"\"\"\n",
    "        return {\n",
    "            'lateral_reading': {\n",
    "                'title': 'Lateral Reading Techniques',\n",
    "                'content': 'Learn to verify information by opening multiple tabs and cross-referencing sources',\n",
    "                'exercises': [\n",
    "                    'Practice identifying authoritative sources',\n",
    "                    'Compare multiple news reports on the same event',\n",
    "                    'Trace information back to original sources'\n",
    "                ]\n",
    "            },\n",
    "            'sift_method': {\n",
    "                'title': 'SIFT Method (Stop, Investigate, Find, Trace)',\n",
    "                'content': 'Four-step process for evaluating online information',\n",
    "                'exercises': [\n",
    "                    'Stop and check your emotional reactions',\n",
    "                    'Investigate the source',\n",
    "                    'Find better coverage',\n",
    "                    'Trace claims to their origin'\n",
    "                ]\n",
    "            },\n",
    "            'reverse_search': {\n",
    "                'title': 'Reverse Image and Video Search',\n",
    "                'content': 'Techniques for verifying visual content',\n",
    "                'exercises': [\n",
    "                    'Use Google reverse image search',\n",
    "                    'Analyze metadata in images',\n",
    "                    'Identify recycled or manipulated content'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_literacy_assessment(self, user_responses=None):\n",
    "        \"\"\"Create personalized digital literacy assessment\"\"\"\n",
    "        if user_responses is None:\n",
    "            # Simulate user responses for demonstration\n",
    "            user_responses = {\n",
    "                'source_evaluation': [0.8, 0.6, 0.7],  # Scores out of 1.0\n",
    "                'bias_recognition': [0.5, 0.7, 0.4],\n",
    "                'verification_skills': [0.6, 0.3, 0.8],\n",
    "                'media_analysis': [0.4, 0.5, 0.6]\n",
    "            }\n",
    "        \n",
    "        # Calculate category scores\n",
    "        category_scores = {}\n",
    "        for category, scores in user_responses.items():\n",
    "            category_scores[category] = np.mean(scores)\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = []\n",
    "        for category, score in category_scores.items():\n",
    "            if score < 0.6:\n",
    "                recommendations.append(f\"Focus on improving {category.replace('_', ' ')}\")\n",
    "        \n",
    "        assessment_result = {\n",
    "            'overall_score': np.mean(list(category_scores.values())),\n",
    "            'category_scores': category_scores,\n",
    "            'recommendations': recommendations,\n",
    "            'suggested_modules': [module for category in recommendations \n",
    "                                for module in self.education_modules.keys()]\n",
    "        }\n",
    "        \n",
    "        return assessment_result\n",
    "    \n",
    "    def generate_training_scenarios(self, difficulty='intermediate'):\n",
    "        \"\"\"Generate training scenarios based on real misinformation patterns\"\"\"\n",
    "        scenarios = {\n",
    "            'beginner': [\n",
    "                {\n",
    "                    'scenario': 'Viral social media post with emotional language',\n",
    "                    'task': 'Identify emotional manipulation techniques',\n",
    "                    'learning_objective': 'Recognize emotional appeals in misinformation'\n",
    "                },\n",
    "                {\n",
    "                    'scenario': 'News article from unknown website',\n",
    "                    'task': 'Evaluate source credibility',\n",
    "                    'learning_objective': 'Learn to assess source reliability'\n",
    "                }\n",
    "            ],\n",
    "            'intermediate': [\n",
    "                {\n",
    "                    'scenario': 'Coordinated hashtag campaign across platforms',\n",
    "                    'task': 'Identify signs of coordinated inauthentic behavior',\n",
    "                    'learning_objective': 'Detect organized misinformation campaigns'\n",
    "                },\n",
    "                {\n",
    "                    'scenario': 'Scientific claim with cherry-picked data',\n",
    "                    'task': 'Verify scientific claims and statistics',\n",
    "                    'learning_objective': 'Evaluate scientific information critically'\n",
    "                }\n",
    "            ],\n",
    "            'advanced': [\n",
    "                {\n",
    "                    'scenario': 'Deepfake video or manipulated image',\n",
    "                    'task': 'Use technical tools to verify media authenticity',\n",
    "                    'learning_objective': 'Master advanced verification techniques'\n",
    "                },\n",
    "                {\n",
    "                    'scenario': 'Cross-platform narrative manipulation',\n",
    "                    'task': 'Track information flow across multiple platforms',\n",
    "                    'learning_objective': 'Understand complex information ecosystems'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return scenarios.get(difficulty, scenarios['intermediate'])\n",
    "\n",
    "# Initialize digital literacy toolkit\n",
    "literacy_toolkit = DigitalLiteracyToolkit()\n",
    "sample_assessment = literacy_toolkit.create_literacy_assessment()\n",
    "training_scenarios = literacy_toolkit.generate_training_scenarios('intermediate')\n",
    "\n",
    "print(\"ğŸ“ Digital Literacy Toolkit Initialized\")\n",
    "print(f\"ğŸ“Š Sample Assessment Score: {sample_assessment['overall_score']:.2f}/1.0\")\n",
    "print(f\"ğŸ’¡ Recommendations: {', '.join(sample_assessment['recommendations'])}\")\n",
    "print(f\"ğŸ“š Generated {len(training_scenarios)} training scenarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Infodemic Management and Response Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfodemicResponseSystem:\n",
    "    \"\"\"Comprehensive system for managing information epidemics\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.alert_thresholds = self._set_alert_thresholds()\n",
    "        self.response_protocols = self._create_response_protocols()\n",
    "        \n",
    "    def _set_alert_thresholds(self):\n",
    "        \"\"\"Define thresholds for different alert levels\"\"\"\n",
    "        return {\n",
    "            'low': {'viral_score': 0.3, 'engagement_rate': 100, 'time_window': 24},\n",
    "            'medium': {'viral_score': 0.6, 'engagement_rate': 500, 'time_window': 12},\n",
    "            'high': {'viral_score': 0.8, 'engagement_rate': 1000, 'time_window': 6},\n",
    "            'critical': {'viral_score': 0.9, 'engagement_rate': 2000, 'time_window': 3}\n",
    "        }\n",
    "    \n",
    "    def _create_response_protocols(self):\n",
    "        \"\"\"Define response protocols for different alert levels\"\"\"\n",
    "        return {\n",
    "            'low': {\n",
    "                'actions': ['Monitor closely', 'Prepare fact-check resources'],\n",
    "                'timeline': '24-48 hours',\n",
    "                'resources': ['Content moderators', 'Fact-checkers']\n",
    "            },\n",
    "            'medium': {\n",
    "                'actions': ['Deploy fact-checkers', 'Issue corrections', 'Monitor spread'],\n",
    "                'timeline': '12-24 hours',\n",
    "                'resources': ['Fact-checking team', 'Communication specialists']\n",
    "            },\n",
    "            'high': {\n",
    "                'actions': ['Immediate fact-check', 'Platform notifications', 'Media outreach'],\n",
    "                'timeline': '6-12 hours',\n",
    "                'resources': ['Full response team', 'Media relations', 'Platform liaisons']\n",
    "            },\n",
    "            'critical': {\n",
    "                'actions': ['Emergency response', 'Platform interventions', 'Public statements'],\n",
    "                'timeline': '1-6 hours',\n",
    "                'resources': ['Crisis team', 'Senior leadership', 'Emergency protocols']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def detect_emerging_infodemics(self, time_window_hours=6):\n",
    "        \"\"\"Detect emerging information epidemics\"\"\"\n",
    "        current_time = self.data['timestamp'].max()\n",
    "        recent_cutoff = current_time - timedelta(hours=time_window_hours)\n",
    "        recent_data = self.data[self.data['timestamp'] >= recent_cutoff]\n",
    "        \n",
    "        # Analyze hashtag velocity and engagement\n",
    "        hashtag_analysis = []\n",
    "        all_hashtags = set()\n",
    "        for hashtag_list in recent_data['hashtag_text']:\n",
    "            all_hashtags.update(hashtag_list)\n",
    "        \n",
    "        for hashtag in all_hashtags:\n",
    "            hashtag_posts = recent_data[recent_data['hashtag_text'].apply(lambda x: hashtag in x)]\n",
    "            \n",
    "            if len(hashtag_posts) >= 5:  # Minimum threshold for analysis\n",
    "                metrics = {\n",
    "                    'hashtag': hashtag,\n",
    "                    'post_count': len(hashtag_posts),\n",
    "                    'total_engagement': hashtag_posts['engagement_score'].sum(),\n",
    "                    'avg_viral_score': hashtag_posts['viral_score'].mean(),\n",
    "                    'platform_diversity': hashtag_posts['platform'].nunique(),\n",
    "                    'high_risk_percentage': (hashtag_posts['misinformation_risk'] == 'high').mean(),\n",
    "                    'time_span_hours': (hashtag_posts['timestamp'].max() - \n",
    "                                      hashtag_posts['timestamp'].min()).total_seconds() / 3600\n",
    "                }\n",
    "                \n",
    "                # Calculate infodemic risk score\n",
    "                metrics['infodemic_risk'] = self._calculate_infodemic_risk(metrics)\n",
    "                metrics['alert_level'] = self._determine_alert_level(metrics)\n",
    "                \n",
    "                hashtag_analysis.append(metrics)\n",
    "        \n",
    "        return pd.DataFrame(hashtag_analysis).sort_values('infodemic_risk', ascending=False)\n",
    "    \n",
    "    def _calculate_infodemic_risk(self, metrics):\n",
    "        \"\"\"Calculate comprehensive infodemic risk score\"\"\"\n",
    "        risk_factors = {\n",
    "            'velocity': min(metrics['post_count'] / max(metrics['time_span_hours'], 1), 10) / 10,\n",
    "            'engagement': min(metrics['total_engagement'] / 10000, 1),\n",
    "            'viral_potential': metrics['avg_viral_score'],\n",
    "            'cross_platform': min(metrics['platform_diversity'] / 5, 1),\n",
    "            'misinformation_content': metrics['high_risk_percentage']\n",
    "        }\n",
    "        \n",
    "        return sum(risk_factors.values()) / len(risk_factors)\n",
    "    \n",
    "    def _determine_alert_level(self, metrics):\n",
    "        \"\"\"Determine alert level based on metrics\"\"\"\n",
    "        risk_score = metrics['infodemic_risk']\n",
    "        \n",
    "        if risk_score >= 0.8:\n",
    "            return 'critical'\n",
    "        elif risk_score >= 0.6:\n",
    "            return 'high'\n",
    "        elif risk_score >= 0.4:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'low'\n",
    "    \n",
    "    def generate_response_plan(self, infodemic_analysis):\n",
    "        \"\"\"Generate comprehensive response plan for detected infodemics\"\"\"\n",
    "        response_plans = []\n",
    "        \n",
    "        for _, infodemic in infodemic_analysis.iterrows():\n",
    "            alert_level = infodemic['alert_level']\n",
    "            protocol = self.response_protocols[alert_level]\n",
    "            \n",
    "            plan = {\n",
    "                'hashtag': infodemic['hashtag'],\n",
    "                'alert_level': alert_level,\n",
    "                'risk_score': infodemic['infodemic_risk'],\n",
    "                'immediate_actions': protocol['actions'],\n",
    "                'response_timeline': protocol['timeline'],\n",
    "                'required_resources': protocol['resources'],\n",
    "                'monitoring_metrics': [\n",
    "                    'Engagement rate changes',\n",
    "                    'Cross-platform spread',\n",
    "                    'Fact-check effectiveness',\n",
    "                    'Public sentiment shifts'\n",
    "                ],\n",
    "                'success_criteria': self._define_success_criteria(alert_level)\n",
    "            }\n",
    "            response_plans.append(plan)\n",
    "        \n",
    "        return response_plans\n",
    "    \n",
    "    def _define_success_criteria(self, alert_level):\n",
    "        \"\"\"Define success criteria for different alert levels\"\"\"\n",
    "        criteria = {\n",
    "            'low': ['Contained spread', 'No escalation'],\n",
    "            'medium': ['50% reduction in sharing', 'Fact-check visibility'],\n",
    "            'high': ['70% reduction in engagement', 'Platform warnings deployed'],\n",
    "            'critical': ['90% reduction in spread', 'Public awareness achieved']\n",
    "        }\n",
    "        return criteria.get(alert_level, criteria['medium'])\n",
    "    \n",
    "    def create_monitoring_dashboard_data(self):\n",
    "        \"\"\"Create data for real-time monitoring dashboard\"\"\"\n",
    "        dashboard_data = {\n",
    "            'real_time_metrics': {\n",
    "                'total_posts_24h': len(self.data[self.data['timestamp'] >= \n",
    "                                      self.data['timestamp'].max() - timedelta(hours=24)]),\n",
    "                'high_risk_posts': len(self.data[self.data['misinformation_risk'] == 'high']),\n",
    "                'fact_check_backlog': len(self.data[~self.data['fact_checked']]),\n",
    "                'active_platforms': self.data['platform'].nunique()\n",
    "            },\n",
    "            'trend_analysis': {\n",
    "                'hashtag_velocity': self._calculate_hashtag_velocity(),\n",
    "                'engagement_trends': self._analyze_engagement_trends(),\n",
    "                'platform_distribution': self.data['platform'].value_counts().to_dict()\n",
    "            },\n",
    "            'alert_summary': {\n",
    "                'active_alerts': 0,  # Would be calculated from current infodemics\n",
    "                'critical_issues': 0,\n",
    "                'resolved_today': 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return dashboard_data\n",
    "    \n",
    "    def _calculate_hashtag_velocity(self):\n",
    "        \"\"\"Calculate hashtag posting velocity\"\"\"\n",
    "        recent_24h = self.data[self.data['timestamp'] >= \n",
    "                              self.data['timestamp'].max() - timedelta(hours=24)]\n",
    "        return len(recent_24h) / 24  # Posts per hour\n",
    "    \n",
    "    def _analyze_engagement_trends(self):\n",
    "        \"\"\"Analyze engagement trends over time\"\"\"\n",
    "        hourly_engagement = self.data.groupby(self.data['timestamp'].dt.hour)['engagement_score'].mean()\n",
    "        return hourly_engagement.to_dict()\n",
    "\n",
    "# Initialize infodemic response system\n",
    "response_system = InfodemicResponseSystem(processed_data)\n",
    "emerging_infodemics = response_system.detect_emerging_infodemics()\n",
    "response_plans = response_system.generate_response_plan(emerging_infodemics.head(5))\n",
    "dashboard_data = response_system.create_monitoring_dashboard_data()\n",
    "\n",
    "print(\"ğŸš¨ Infodemic Response System Activated\")\n",
    "print(f\"ğŸ“Š Detected {len(emerging_infodemics)} potential infodemics\")\n",
    "print(f\"ğŸ“‹ Generated {len(response_plans)} response plans\")\n",
    "print(f\"\\nâš ï¸  Alert Summary:\")\n",
    "if len(emerging_infodemics) > 0:\n",
    "    print(emerging_infodemics['alert_level'].value_counts())\n",
    "else:\n",
    "    print(\"No current alerts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization and Reporting Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_visualizations():\n",
    "    \"\"\"Create comprehensive visualization dashboard\"\"\"\n",
    "    \n",
    "    # Set up the plotting area\n",
    "    fig = plt.figure(figsize=(20, 24))\n",
    "    \n",
    "    # 1. Hashtag Trend Analysis\n",
    "    plt.subplot(4, 3, 1)\n",
    "    top_hashtags = trending_hashtags.head(10)\n",
    "    plt.barh(top_hashtags['hashtag'], top_hashtags['frequency'])\n",
    "    plt.title('Top 10 Trending Hashtags', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Frequency')\n",
    "    \n",
    "    # 2. Misinformation Risk Distribution\n",
    "    plt.subplot(4, 3, 2)\n",
    "    risk_counts = processed_data['misinformation_risk'].value_counts()\n",
    "    plt.pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%')\n",
    "    plt.title('Misinformation Risk Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 3. Platform Distribution\n",
    "    plt.subplot(4, 3, 3)\n",
    "    platform_counts = processed_data['platform'].value_counts()\n",
    "    plt.bar(platform_counts.index, platform_counts.values)\n",
    "    plt.title('Posts by Platform', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 4. Engagement vs Credibility Scatter\n",
    "    plt.subplot(4, 3, 4)\n",
    "    scatter = plt.scatter(processed_data['credibility_score'], \n",
    "                         processed_data['engagement_score'],\n",
    "                         c=processed_data['viral_score'], \n",
    "                         alpha=0.6, cmap='viridis')\n",
    "    plt.xlabel('Credibility Score')\n",
    "    plt.ylabel('Engagement Score')\n",
    "    plt.title('Engagement vs Credibility', fontsize=14, fontweight='bold')\n",
    "    plt.colorbar(scatter, label='Viral Score')\n",
    "    \n",
    "    # 5. Timeline of Posts\n",
    "    plt.subplot(4, 3, 5)\n",
    "    daily_posts = processed_data.groupby('date').size()\n",
    "    plt.plot(daily_posts.index, daily_posts.values, marker='o')\n",
    "    plt.title('Daily Post Volume', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Posts')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 6. Fact-Check Status\n",
    "    plt.subplot(4, 3, 6)\n",
    "    fact_check_status = processed_data['fact_checked'].value_counts()\n",
    "    plt.pie(fact_check_status.values, \n",
    "            labels=['Not Fact-Checked', 'Fact-Checked'], \n",
    "            autopct='%1.1f%%')\n",
    "    plt.title('Fact-Check Coverage', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 7. Network Influence Analysis\n",
    "    plt.subplot(4, 3, 7)\n",
    "    top_influential = influence_metrics.head(10)\n",
    "    plt.barh(top_influential['hashtag'], top_influential['composite_influence'])\n",
    "    plt.title('Most Influential Hashtags', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Influence Score')\n",
    "    \n",
    "    # 8. Verification Results Distribution\n",
    "    plt.subplot(4, 3, 8)\n",
    "    if len(verification_results) > 0:\n",
    "        status_counts = verification_results['status'].value_counts()\n",
    "        plt.bar(status_counts.index, status_counts.values)\n",
    "        plt.title('Verification Results', fontsize=14, fontweight='bold')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    # 9. Hourly Activity Heatmap\n",
    "    plt.subplot(4, 3, 9)\n",
    "    hourly_activity = processed_data.groupby(['day_of_week', 'hour']).size().unstack(fill_value=0)\n",
    "    sns.heatmap(hourly_activity, cmap='YlOrRd', cbar=True)\n",
    "    plt.title('Activity Heatmap (Day vs Hour)', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Day of Week')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    \n",
    "    # 10. Campaign Risk Analysis\n",
    "    plt.subplot(4, 3, 10)\n",
    "    if len(potential_campaigns) > 0:\n",
    "        plt.scatter(potential_campaigns['post_count'], \n",
    "                   potential_campaigns['risk_score'],\n",
    "                   alpha=0.7)\n",
    "        plt.xlabel('Campaign Size (Posts)')\n",
    "        plt.ylabel('Risk Score')\n",
    "        plt.title('Campaign Risk Analysis', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 11. Digital Literacy Assessment\n",
    "    plt.subplot(4, 3, 11)\n",
    "    categories = list(sample_assessment['category_scores'].keys())\n",
    "    scores = list(sample_assessment['category_scores'].values())\n",
    "    plt.bar(range(len(categories)), scores)\n",
    "    plt.xticks(range(len(categories)), [cat.replace('_', '\\n') for cat in categories], \n",
    "               rotation=45, ha='right')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Digital Literacy Assessment', fontsize=14, fontweight='bold')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # 12. Infodemic Alert Levels\n",
    "    plt.subplot(4, 3, 12)\n",
    "    if len(emerging_infodemics) > 0:\n",
    "        alert_counts = emerging_infodemics['alert_level'].value_counts()\n",
    "        colors = {'low': 'green', 'medium': 'yellow', 'high': 'orange', 'critical': 'red'}\n",
    "        plt.bar(alert_counts.index, alert_counts.values, 
               color=[colors.get(level, 'gray') for level in alert_counts.index])
        plt.title('Infodemic Alert Levels', fontsize=14, fontweight='bold')
        plt.ylabel('Count')
    else:
        plt.text(0.5, 0.5, 'No Active Alerts', ha='center', va='center', 
                transform=plt.gca().transAxes, fontsize=12)
        plt.title('Infodemic Alert Levels', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.show()

def create_network_visualization():
    """Create network visualization of hashtag relationships"""
    if flow_analyzer.network is None:
        return
    
    # Create network layout
    pos = nx.spring_layout(flow_analyzer.network, k=1, iterations=50)
    
    # Set up the plot
    plt.figure(figsize=(15, 12))
    
    # Draw network
    # Node sizes based on frequency
    node_sizes = [flow_analyzer.network.nodes[node]['weight'] * 50 
                  for node in flow_analyzer.network.nodes()]
    
    # Edge widths based on co-occurrence strength
    edge_widths = [flow_analyzer.network[u][v]['weight'] * 0.5 
                   for u, v in flow_analyzer.network.edges()]
    
    # Draw the network
    nx.draw_networkx_nodes(flow_analyzer.network, pos, 
                          node_size=node_sizes, 
                          node_color='lightblue', 
                          alpha=0.7)
    
    nx.draw_networkx_edges(flow_analyzer.network, pos, 
                          width=edge_widths, 
                          alpha=0.5, 
                          edge_color='gray')
    
    # Add labels for most important nodes
    important_nodes = influence_metrics.head(15)['hashtag'].tolist()
    important_labels = {node: node for node in important_nodes 
                       if node in flow_analyzer.network.nodes()}
    
    nx.draw_networkx_labels(flow_analyzer.network, pos, 
                           important_labels, 
                           font_size=8)
    
    plt.title('Hashtag Co-occurrence Network', fontsize=16, fontweight='bold')
    plt.axis('off')
    plt.tight_layout()
    plt.show()

def generate_executive_summary():
    """Generate executive summary report"""
    summary = f"""
    # MISINFORMATION TRACKING EXECUTIVE SUMMARY
    ## Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}
    
    ### KEY METRICS
    - **Total Posts Analyzed**: {len(processed_data):,}
    - **High-Risk Content**: {len(processed_data[processed_data['misinformation_risk'] == 'high']):,} ({(len(processed_data[processed_data['misinformation_risk'] == 'high'])/len(processed_data)*100):.1f}%)
    - **Fact-Check Coverage**: {(processed_data['fact_checked'].sum()/len(processed_data)*100):.1f}%
    - **Active Platforms**: {processed_data['platform'].nunique()}
    
    ### TRENDING MISINFORMATION INDICATORS
    - **Top Concerning Hashtag**: #{trending_hashtags.iloc[0]['hashtag']} ({trending_hashtags.iloc[0]['frequency']} posts)
    - **Most Influential Spreader**: #{influence_metrics.iloc[0]['hashtag']} (influence score: {influence_metrics.iloc[0]['composite_influence']:.3f})
    - **Bridge Narratives**: {len(bridge_hashtags)} hashtags connecting different misinformation themes
    
    ### CAMPAIGN ANALYSIS
    - **Potential Coordinated Campaigns**: {len(potential_campaigns)}
    - **High-Risk Campaigns**: {len(potential_campaigns[potential_campaigns['risk_score'] > 0.7]) if len(potential_campaigns) > 0 else 0}
    
    ### VERIFICATION STATUS
    """
    
    if len(verification_results) > 0:
        false_content = len(verification_results[verification_results['status'] == 'False'])
        disputed_content = len(verification_results[verification_results['status'] == 'Disputed'])
        summary += f"""
    - **False Content Identified**: {false_content} posts
    - **Disputed Claims**: {disputed_content} posts
    - **Recommended Takedowns**: {len(verification_results[verification_results['recommended_action'] == 'Immediate Takedown'])}
        """
    
    summary += f"""
    
    ### INFODEMIC ALERTS
    - **Active Alerts**: {len(emerging_infodemics)}
    """
    
    if len(emerging_infodemics) > 0:
        critical_alerts = len(emerging_infodemics[emerging_infodemics['alert_level'] == 'critical'])
        high_alerts = len(emerging_infodemics[emerging_infodemics['alert_level'] == 'high'])
        summary += f"""
    - **Critical Level**: {critical_alerts}
    - **High Level**: {high_alerts}
    - **Top Risk Hashtag**: #{emerging_infodemics.iloc[0]['hashtag']} (risk score: {emerging_infodemics.iloc[0]['infodemic_risk']:.3f})
        """
    
    summary += f"""
    
    ### DIGITAL LITERACY ASSESSMENT
    - **Overall Population Score**: {sample_assessment['overall_score']:.2f}/1.0
    - **Key Improvement Areas**: {', '.join(sample_assessment['recommendations'][:3]) if sample_assessment['recommendations'] else 'None identified'}
    
    ### RECOMMENDATIONS
    1. **Immediate Actions**: Focus fact-checking resources on {len(fact_check_queue.head(20))} highest-priority posts
    2. **Platform Coordination**: Engage with platforms regarding {len(potential_campaigns[potential_campaigns['risk_score'] > 0.6]) if len(potential_campaigns) > 0 else 0} suspicious campaigns
    3. **Public Education**: Deploy digital literacy modules focusing on {sample_assessment['recommendations'][0].replace('Focus on improving ', '') if sample_assessment['recommendations'] else 'general verification skills'}
    4. **Monitoring Enhancement**: Increase surveillance of #{influence_metrics.iloc[0]['hashtag']} network cluster
    
    ### NEXT REVIEW
    - **Recommended Review Frequency**: Every 6 hours during high-alert periods
    - **Next Scheduled Analysis**: {(datetime.now() + timedelta(hours=6)).strftime('%Y-%m-%d %H:%M')}
    """
    
    return summary

# Generate visualizations and summary
print("ğŸ“Š Creating Comprehensive Visualizations...")
create_comprehensive_visualizations()

print("ğŸŒ Creating Network Visualization...")
create_network_visualization()

print("ğŸ“‹ Generating Executive Summary...")
executive_summary = generate_executive_summary()
print(executive_summary)\n{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misinformation Hashtag Tracker\n",
    "## Information Access Specialist Toolkit\n",
    "\n",
    "This notebook provides comprehensive tools for tracking, analyzing, and managing misinformation spread through social media networks. It focuses on hashtag analysis, narrative propagation detection, and \"infodemic\" management strategies.\n",
    "\n",
    "**Key Capabilities:**\n",
    "- Social media misinformation tracking\n",
    "- Hashtag trend analysis and narrative mapping\n",
    "- Network analysis of information flow\n",
    "- Content verification and fact-checking workflows\n",
    "- Digital literacy assessment tools\n",
    "- Infodemic response strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Network analysis\n",
    "import networkx as nx\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Text processing and NLP\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Machine learning for classification\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set up plotting parameters\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection and Preprocessing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialMediaCollector:\n",
    "    \"\"\"Framework for collecting social media data from multiple platforms\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.platforms = ['twitter', 'facebook', 'instagram', 'tiktok', 'telegram']\n",
    "        self.collected_data = []\n",
    "        \n",
    "    def simulate_data_collection(self, num_posts=1000):\n",
    "        \"\"\"Simulate social media data collection for demonstration\"\"\"\n",
    "        import random\n",
    "        \n",
    "        # Sample misinformation-related hashtags and keywords\n",
    "        misinfo_hashtags = [\n",
    "            '#fakenews', '#conspiracy', '#hoax', '#scam', '#lies',\n",
    "            '#coverup', '#agenda', '#propaganda', '#manufactured',\n",
    "            '#staged', '#crisis', '#actors', '#false', '#planted'\n",
    "        ]\n",
    "        \n",
    "        # Sample legitimate hashtags that might get mixed in\n",
    "        mixed_hashtags = [\n",
    "            '#news', '#breaking', '#update', '#truth', '#facts',\n",
    "            '#investigation', '#report', '#analysis', '#verified'\n",
    "        ]\n",
    "        \n",
    "        all_hashtags = misinfo_hashtags + mixed_hashtags\n",
    "        \n",
    "        data = []\n",
    "        base_date = datetime.now() - timedelta(days=30)\n",
    "        \n",
    "        for i in range(num_posts):\n",
    "            # Generate synthetic post data\n",
    "            num_hashtags = random.randint(1, 5)\n",
    "            post_hashtags = random.sample(all_hashtags, num_hashtags)\n",
    "            \n",
    "            post = {\n",
    "                'post_id': f'post_{i:06d}',\n",
    "                'platform': random.choice(self.platforms),\n",
    "                'timestamp': base_date + timedelta(\n",
    "                    days=random.randint(0, 30),\n",
    "                    hours=random.randint(0, 23),\n",
    "                    minutes=random.randint(0, 59)\n",
    "                ),\n",
    "                'hashtags': post_hashtags,\n",
    "                'engagement_score': random.randint(1, 10000),\n",
    "                'shares': random.randint(0, 5000),\n",
    "                'likes': random.randint(0, 20000),\n",
    "                'comments': random.randint(0, 1000),\n",
    "                'user_followers': random.randint(100, 100000),\n",
    "                'verified_user': random.choice([True, False]),\n",
    "                'content_length': random.randint(50, 280),\n",
    "                'contains_media': random.choice([True, False]),\n",
    "                'language': random.choice(['en', 'es', 'fr', 'de', 'pt']),\n",
    "                'credibility_score': random.uniform(0, 1),  # 0 = low credibility, 1 = high\n",
    "                'fact_checked': random.choice([True, False]),\n",
    "                'misinformation_risk': random.choice(['low', 'medium', 'high'])\n",
    "            }\n",
    "            data.append(post)\n",
    "        \n",
    "        self.collected_data = pd.DataFrame(data)\n",
    "        return self.collected_data\n",
    "    \n",
    "    def preprocess_data(self, df):\n",
    "        \"\"\"Clean and preprocess collected social media data\"\"\"\n",
    "        # Convert timestamp to datetime\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        # Extract hashtag text without # symbol\n",
    "        df['hashtag_text'] = df['hashtags'].apply(\n",
    "            lambda x: [tag.replace('#', '').lower() for tag in x]\n",
    "        )\n",
    "        \n",
    "        # Calculate viral potential score\n",
    "        df['viral_score'] = (\n",
    "            df['shares'] * 0.4 + \n",
    "            df['likes'] * 0.3 + \n",
    "            df['comments'] * 0.3\n",
    "        ) / df['user_followers']\n",
    "        \n",
    "        # Add time-based features\n",
    "        df['hour'] = df['timestamp'].dt.hour\n",
    "        df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Initialize collector and generate sample data\n",
    "collector = SocialMediaCollector()\n",
    "sample_data = collector.simulate_data_collection(1500)\n",
    "processed_data = collector.preprocess_data(sample_data.copy())\n",
    "\n",
    "print(f\"âœ… Collected and processed {len(processed_data)} social media posts\")\n",
    "print(f\"ğŸ“Š Data shape: {processed_data.shape}\")\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hashtag Analysis and Narrative Propagation Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashtagAnalyzer:\n",
    "    \"\"\"Comprehensive hashtag analysis for misinformation tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.hashtag_trends = None\n",
    "        self.narrative_clusters = None\n",
    "        \n",
    "    def analyze_hashtag_frequency(self):\n",
    "        \"\"\"Analyze hashtag frequency and trending patterns\"\"\"\n",
    "        # Flatten all hashtags\n",
    "        all_hashtags = []\n",
    "        for hashtag_list in self.data['hashtag_text']:\n",
    "            all_hashtags.extend(hashtag_list)\n",
    "        \n",
    "        # Count frequencies\n",
    "        hashtag_counts = Counter(all_hashtags)\n",
    "        \n",
    "        # Create trending analysis\n",
    "        trending_df = pd.DataFrame([\n",
    "            {'hashtag': tag, 'frequency': count, 'percentage': (count/len(all_hashtags))*100}\n",
    "            for tag, count in hashtag_counts.most_common(50)\n",
    "        ])\n",
    "        \n",
    "        self.hashtag_trends = trending_df\n",
    "        return trending_df\n",
    "    \n",
    "    def detect_coordinated_campaigns(self, time_window_hours=6, min_posts=10):\n",
    "        \"\"\"Detect potential coordinated misinformation campaigns\"\"\"\n",
    "        campaigns = []\n",
    "        \n",
    "        # Group by hashtag combinations\n",
    "        hashtag_combos = self.data['hashtags'].apply(lambda x: tuple(sorted(x)))\n",
    "        combo_groups = self.data.groupby(hashtag_combos)\n",
    "        \n",
    "        for combo, group in combo_groups:\n",
    "            if len(group) >= min_posts:\n",
    "                # Check if posts are clustered in time\n",
    "                time_diffs = group['timestamp'].diff().dt.total_seconds() / 3600\n",
    "                clustered_posts = (time_diffs <= time_window_hours).sum()\n",
    "                \n",
    "                if clustered_posts >= min_posts * 0.7:  # 70% of posts clustered\n",
    "                    campaign_info = {\n",
    "                        'hashtag_combo': combo,\n",
    "                        'post_count': len(group),\n",
    "                        'time_span_hours': (group['timestamp'].max() - group['timestamp'].min()).total_seconds() / 3600,\n",
    "                        'avg_engagement': group['engagement_score'].mean(),\n",
    "                        'platforms': group['platform'].unique().tolist(),\n",
    "                        'risk_score': self._calculate_campaign_risk(group)\n",
    "                    }\n",
    "                    campaigns.append(campaign_info)\n",
    "        \n",
    "        return pd.DataFrame(campaigns).sort_values('risk_score', ascending=False)\n",
    "    \n",
    "    def _calculate_campaign_risk(self, group):\n",
    "        \"\"\"Calculate risk score for potential misinformation campaign\"\"\"\n",
    "        risk_factors = {\n",
    "            'high_misinformation_risk': (group['misinformation_risk'] == 'high').sum() / len(group),\n",
    "            'low_credibility': (group['credibility_score'] < 0.3).sum() / len(group),\n",
    "            'not_fact_checked': (~group['fact_checked']).sum() / len(group),\n",
    "            'rapid_spread': (group['viral_score'] > group['viral_score'].quantile(0.8)).sum() / len(group)\n",
    "        }\n",
    "        \n",
    "        return sum(risk_factors.values()) / len(risk_factors)\n",
    "    \n",
    "    def create_narrative_clusters(self, n_clusters=8):\n",
    "        \"\"\"Cluster hashtags to identify narrative themes\"\"\"\n",
    "        # Create hashtag co-occurrence matrix\n",
    "        all_hashtags = set()\n",
    "        for hashtag_list in self.data['hashtag_text']:\n",
    "            all_hashtags.update(hashtag_list)\n",
    "        \n",
    "        hashtag_list = list(all_hashtags)\n",
    "        cooccurrence_matrix = np.zeros((len(hashtag_list), len(hashtag_list)))\n",
    "        \n",
    "        for hashtags in self.data['hashtag_text']:\n",
    "            for i, tag1 in enumerate(hashtag_list):\n",
    "                for j, tag2 in enumerate(hashtag_list):\n",
    "                    if tag1 in hashtags and tag2 in hashtags:\n",
    "                        cooccurrence_matrix[i][j] += 1\n",
    "        \n",
    "        # Apply clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        clusters = kmeans.fit_predict(cooccurrence_matrix)\n",
    "        \n",
    "        # Create cluster DataFrame\n",
    "        cluster_df = pd.DataFrame({\n",
    "            'hashtag': hashtag_list,\n",
    "            'cluster': clusters\n",
    "        })\n",
    "        \n",
    "        self.narrative_clusters = cluster_df\n",
    "        return cluster_df\n",
    "\n",
    "# Perform hashtag analysis\n",
    "analyzer = HashtagAnalyzer(processed_data)\n",
    "trending_hashtags = analyzer.analyze_hashtag_frequency()\n",
    "potential_campaigns = analyzer.detect_coordinated_campaigns()\n",
    "narrative_clusters = analyzer.create_narrative_clusters()\n",
    "\n",
    "print(\"ğŸ” Hashtag Analysis Complete\")\n",
    "print(f\"ğŸ“ˆ Top 10 trending hashtags:\")\n",
    "print(trending_hashtags.head(10))\n",
    "print(f\"\\nâš ï¸  Detected {len(potential_campaigns)} potential coordinated campaigns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Network Analysis of Information Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformationFlowAnalyzer:\n",
    "    \"\"\"Network analysis of how misinformation spreads through social networks\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.network = None\n",
    "        self.influence_scores = None\n",
    "        \n",
    "    def build_hashtag_network(self):\n",
    "        \"\"\"Build network graph of hashtag co-occurrences\"\"\"\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes and edges based on hashtag co-occurrences\n",
    "        for hashtags in self.data['hashtag_text']:\n",
    "            # Add nodes\n",
    "            for hashtag in hashtags:\n",
    "                if not G.has_node(hashtag):\n",
    "                    G.add_node(hashtag, weight=1)\n",
    "                else:\n",
    "                    G.nodes[hashtag]['weight'] += 1\n",
    "            \n",
    "            # Add edges for co-occurrences\n",
    "            for i in range(len(hashtags)):\n",
    "                for j in range(i+1, len(hashtags)):\n",
    "                    tag1, tag2 = hashtags[i], hashtags[j]\n",
    "                    if G.has_edge(tag1, tag2):\n",
    "                        G[tag1][tag2]['weight'] += 1\n",
    "                    else:\n",
    "                        G.add_edge(tag1, tag2, weight=1)\n",
    "        \n",
    "        self.network = G\n",
    "        return G\n",
    "    \n",
    "    def calculate_influence_metrics(self):\n",
    "        \"\"\"Calculate various centrality measures to identify influential hashtags\"\"\"\n",
    "        if self.network is None:\n",
    "            self.build_hashtag_network()\n",
    "        \n",
    "        # Calculate different centrality measures\n",
    "        centrality_measures = {\n",
    "            'betweenness': nx.betweenness_centrality(self.network, weight='weight'),\n",
    "            'closeness': nx.closeness_centrality(self.network),\n",
    "            'degree': nx.degree_centrality(self.network),\n",
    "            'eigenvector': nx.eigenvector_centrality(self.network, weight='weight', max_iter=1000)\n",
    "        }\n",
    "        \n",
    "        # Create comprehensive influence DataFrame\n",
    "        influence_data = []\n",
    "        for hashtag in self.network.nodes():\n",
    "            influence_data.append({\n",
    "                'hashtag': hashtag,\n",
    "                'frequency': self.network.nodes[hashtag]['weight'],\n",
    "                'betweenness_centrality': centrality_measures['betweenness'][hashtag],\n",
    "                'closeness_centrality': centrality_measures['closeness'][hashtag],\n",
    "                'degree_centrality': centrality_measures['degree'][hashtag],\n",
    "                'eigenvector_centrality': centrality_measures['eigenvector'][hashtag]\n",
    "            })\n",
    "        \n",
    "        influence_df = pd.DataFrame(influence_data)\n",
    "        \n",
    "        # Calculate composite influence score\n",
    "        influence_df['composite_influence'] = (\n",
    "            influence_df['betweenness_centrality'] * 0.3 +\n",
    "            influence_df['degree_centrality'] * 0.3 +\n",
    "            influence_df['eigenvector_centrality'] * 0.4\n",
    "        )\n",
    "        \n",
    "        self.influence_scores = influence_df.sort_values('composite_influence', ascending=False)\n",
    "        return self.influence_scores\n",
    "    \n",
    "    def identify_bridge_hashtags(self, top_n=10):\n",
    "        \"\"\"Identify hashtags that bridge different narrative communities\"\"\"\n",
    "        if self.influence_scores is None:\n",
    "            self.calculate_influence_metrics()\n",
    "        \n",
    "        # Bridge hashtags have high betweenness centrality\n",
    "        bridge_hashtags = self.influence_scores.nlargest(top_n, 'betweenness_centrality')\n",
    "        return bridge_hashtags[['hashtag', 'betweenness_centrality', 'composite_influence']]\n",
    "    \n",
    "    def detect_information_bottlenecks(self):\n",
    "        \"\"\"Detect potential information bottlenecks in the network\"\"\"\n",
    "        if self.network is None:\n",
    "            self.build_hashtag_network()\n",
    "        \n",
    "        # Find articulation points (nodes whose removal increases components)\n",
    "        articulation_points = list(nx.articulation_points(self.network))\n",
    "        \n",
    "        # Calculate edge betweenness centrality\n",
    "        edge_betweenness = nx.edge_betweenness_centrality(self.network, weight='weight')\n",
    "        \n",
    "        bottlenecks = {\n",
    "            'critical_hashtags': articulation_points,\n",
    "            'critical_edges': sorted(edge_betweenness.items(), \n",
    "                                   key=lambda x: x[1], reverse=True)[:10]\n",
    "        }\n",
    "        \n",
    "        return bottlenecks\n",
    "\n",
    "# Perform network analysis\n",
    "flow_analyzer = InformationFlowAnalyzer(processed_data)\n",
    "hashtag_network = flow_analyzer.build_hashtag_network()\n",
    "influence_metrics = flow_analyzer.calculate_influence_metrics()\n",
    "bridge_hashtags = flow_analyzer.identify_bridge_hashtags()\n",
    "bottlenecks = flow_analyzer.detect_information_bottlenecks()\n",
    "\n",
    "print(\"ğŸŒ Network Analysis Complete\")\n",
    "print(f\"ğŸ“Š Network stats: {hashtag_network.number_of_nodes()} nodes, {hashtag_network.number_of_edges()} edges\")\n",
    "print(f\"\\nğŸŒ‰ Top 5 bridge hashtags:\")\n",
    "print(bridge_hashtags.head())\n",
    "print(f\"\\nâš ï¸  Found {len(bottlenecks['critical_hashtags'])} critical information bottlenecks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Content Analysis and Fact-Checking Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentVerificationSystem:\n",
    "    \"\"\"Automated content analysis and fact-checking framework\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.credibility_model = None\n",
    "        self.verification_results = None\n",
    "        \n",
    "    def analyze_credibility_signals(self):\n",
    "        \"\"\"Analyze various signals that indicate content credibility\"\"\"\n",
    "        credibility_analysis = self.data.copy()\n",
    "        \n",
    "        # Source credibility indicators\n",
    "        credibility_analysis['verified_source'] = credibility_analysis['verified_user'].astype(int)\n",
    "        credibility_analysis['follower_credibility'] = np.where(\n",
    "            credibility_analysis['user_followers'] > 10000, 1, 0\n",
    "        )\n",
    "        \n",
    "        # Content characteristics\n",
    "        credibility_analysis['optimal_length'] = np.where(\n",
    "            (credibility_analysis['content_length'] >= 50) & \n",
    "            (credibility_analysis['content_length'] <= 200), 1, 0\n",
    "        )\n",
    "        \n",
    "        # Engagement patterns (suspicious if too high too fast)\n",
    "        credibility_analysis['normal_engagement'] = np.where(\n",
    "            credibility_analysis['viral_score'] < credibility_analysis['viral_score'].quantile(0.95), 1, 0\n",
    "        )\n",
    "        \n",
    "        # Media presence (images/videos can be manipulated)\n",
    "        credibility_analysis['media_risk'] = np.where(\n",
    "            credibility_analysis['contains_media'] & \n",
    "            (credibility_analysis['misinformation_risk'] == 'high'), 1, 0\n",
    "        )\n",
    "        \n",
    "        # Calculate composite credibility score\n",
    "        credibility_analysis['auto_credibility_score'] = (\n",
    "            credibility_analysis['verified_source'] * 0.3 +\n",
    "            credibility_analysis['follower_credibility'] * 0.2 +\n",
    "            credibility_analysis['optimal_length'] * 0.1 +\n",
    "            credibility_analysis['normal_engagement'] * 0.2 +\n",
    "            (1 - credibility_analysis['media_risk']) * 0.2\n",
    "        )\n",
    "        \n",
    "        return credibility_analysis\n",
    "    \n",
    "    def create_fact_check_priority_queue(self):\n",
    "        \"\"\"Create priority queue for fact-checking based on risk and reach\"\"\"\n",
    "        credibility_data = self.analyze_credibility_signals()\n",
    "        \n",
    "        # Calculate priority score (higher = more urgent to fact-check)\n",
    "        credibility_data['fact_check_priority'] = (\n",
    "            (1 - credibility_data['auto_credibility_score']) * 0.4 +  # Low credibility = high priority\n",
    "            (credibility_data['viral_score'] / credibility_data['viral_score'].max()) * 0.3 +  # High viral = high priority\n",
    "            (credibility_data['engagement_score'] / credibility_data['engagement_score'].max()) * 0.3  # High engagement = high priority\n",
    "        )\n",
    "        \n",
    "        # Filter for posts not yet fact-checked\n",
    "        priority_queue = credibility_data[~credibility_data['fact_checked']].copy()\n",
    "        priority_queue = priority_queue.sort_values('fact_check_priority', ascending=False)\n",
    "        \n",
    "        return priority_queue[[\n",
    "            'post_id', 'platform', 'hashtags', 'fact_check_priority',\n",
    "            'auto_credibility_score', 'viral_score', 'engagement_score',\n",
    "            'misinformation_risk'\n",
    "        ]]\n",
    "    \n",
    "    def simulate_fact_checking_workflow(self, top_n=20):\n",
    "        \"\"\"Simulate automated fact-checking workflow\"\"\"\n",
    "        priority_queue = self.create_fact_check_priority_queue()\n",
    "        top_priority = priority_queue.head(top_n)\n",
    "        \n",
    "        # Simulate fact-checking results\n",
    "        fact_check_results = []\n",
    "        \n",
    "        for _, post in top_priority.iterrows():\n",
    "            # Simulate fact-checking process\n",
    "            verification_score = np.random.beta(2, 5)  # Skewed toward lower scores\n",
    "            \n",
    "            result = {\n",
    "                'post_id': post['post_id'],\n",
    "                'platform': post['platform'],\n",
    "                'hashtags': post['hashtags'],\n",
    "                'priority_score': post['fact_check_priority'],\n",
    "                'verification_score': verification_score,\n",
    "                'status': self._determine_verification_status(verification_score),\n",
    "                'recommended_action': self._recommend_action(verification_score, post['viral_score'])\n",
    "            }\n",
    "            fact_check_results.append(result)\n",
    "        \n",
    "        self.verification_results = pd.DataFrame(fact_check_results)\n",
    "        return self.verification_results\n",
    "    \n",
    "    def _determine_verification_status(self, score):\n",
    "        \"\"\"Determine verification status based on score\"\"\"\n",
    "        if score >= 0.7:\n",
    "            return 'Verified'\n",
    "        elif score >= 0.4:\n",
    "            return 'Partially Verified'\n",
    "        elif score >= 0.2:\n",
    "            return 'Disputed'\n",
    "        else:\n",
    "            return 'False'\n",
    "    \n",
    "    def _recommend_action(self, verification_score, viral_score):\n",
    "        \"\"\"Recommend action based on verification and viral scores\"\"\"\n",
    "        if verification_score < 0.3 and viral_score > 0.5:\n",
    "            return 'Immediate Takedown'\n",
    "        elif verification_