{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misinformation Hashtag Tracker\n",
    "## Information Access Specialist Toolkit\n",
    "\n",
    "This notebook provides comprehensive tools for tracking, analyzing, and managing misinformation spread through social media networks. It focuses on hashtag analysis, narrative propagation detection, and \"infodemic\" management strategies.\n",
    "\n",
    "**Key Capabilities:**\n",
    "- Social media misinformation tracking\n",
    "- Hashtag trend analysis and narrative mapping\n",
    "- Network analysis of information flow\n",
    "- Content verification and fact-checking workflows\n",
    "- Digital literacy assessment tools\n",
    "- Infodemic response strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Network analysis\n",
    "import networkx as nx\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Text processing and NLP\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Machine learning for classification\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set up plotting parameters\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection and Preprocessing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialMediaCollector:\n",
    "    \"\"\"Framework for collecting social media data from multiple platforms\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.platforms = ['twitter', 'facebook', 'instagram', 'tiktok', 'telegram']\n",
    "        self.collected_data = []\n",
    "        \n",
    "    def simulate_data_collection(self, num_posts=1000):\n",
    "        \"\"\"Simulate social media data collection for demonstration\"\"\"\n",
    "        import random\n",
    "        \n",
    "        # Sample misinformation-related hashtags and keywords\n",
    "        misinfo_hashtags = [\n",
    "            '#fakenews', '#conspiracy', '#hoax', '#scam', '#lies',\n",
    "            '#coverup', '#agenda', '#propaganda', '#manufactured',\n",
    "            '#staged', '#crisis', '#actors', '#false', '#planted'\n",
    "        ]\n",
    "        \n",
    "        # Sample legitimate hashtags that might get mixed in\n",
    "        mixed_hashtags = [\n",
    "            '#news', '#breaking', '#update', '#truth', '#facts',\n",
    "            '#investigation', '#report', '#analysis', '#verified'\n",
    "        ]\n",
    "        \n",
    "        all_hashtags = misinfo_hashtags + mixed_hashtags\n",
    "        \n",
    "        data = []\n",
    "        base_date = datetime.now() - timedelta(days=30)\n",
    "        \n",
    "        for i in range(num_posts):\n",
    "            # Generate synthetic post data\n",
    "            num_hashtags = random.randint(1, 5)\n",
    "            post_hashtags = random.sample(all_hashtags, num_hashtags)\n",
    "            \n",
    "            post = {\n",
    "                'post_id': f'post_{i:06d}',\n",
    "                'platform': random.choice(self.platforms),\n",
    "                'timestamp': base_date + timedelta(\n",
    "                    days=random.randint(0, 30),\n",
    "                    hours=random.randint(0, 23),\n",
    "                    minutes=random.randint(0, 59)\n",
    "                ),\n",
    "                'hashtags': post_hashtags,\n",
    "                'engagement_score': random.randint(1, 10000),\n",
    "                'shares': random.randint(0, 5000),\n",
    "                'likes': random.randint(0, 20000),\n",
    "                'comments': random.randint(0, 1000),\n",
    "                'user_followers': random.randint(100, 100000),\n",
    "                'verified_user': random.choice([True, False]),\n",
    "                'content_length': random.randint(50, 280),\n",
    "                'contains_media': random.choice([True, False]),\n",
    "                'language': random.choice(['en', 'es', 'fr', 'de', 'pt']),\n",
    "                'credibility_score': random.uniform(0, 1),  # 0 = low credibility, 1 = high\n",
    "                'fact_checked': random.choice([True, False]),\n",
    "                'misinformation_risk': random.choice(['low', 'medium', 'high'])\n",
    "            }\n",
    "            data.append(post)\n",
    "        \n",
    "        self.collected_data = pd.DataFrame(data)\n",
    "        return self.collected_data\n",
    "    \n",
    "    def preprocess_data(self, df):\n",
    "        \"\"\"Clean and preprocess collected social media data\"\"\"\n",
    "        # Convert timestamp to datetime\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        # Extract hashtag text without # symbol\n",
    "        df['hashtag_text'] = df['hashtags'].apply(\n",
    "            lambda x: [tag.replace('#', '').lower() for tag in x]\n",
    "        )\n",
    "        \n",
    "        # Calculate viral potential score\n",
    "        df['viral_score'] = (\n",
    "            df['shares'] * 0.4 + \n",
    "            df['likes'] * 0.3 + \n",
    "            df['comments'] * 0.3\n",
    "        ) / df['user_followers']\n",
    "        \n",
    "        # Add time-based features\n",
    "        df['hour'] = df['timestamp'].dt.hour\n",
    "        df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Initialize collector and generate sample data\n",
    "collector = SocialMediaCollector()\n",
    "sample_data = collector.simulate_data_collection(1500)\n",
    "processed_data = collector.preprocess_data(sample_data.copy())\n",
    "\n",
    "print(f\"‚úÖ Collected and processed {len(processed_data)} social media posts\")\n",
    "print(f\"üìä Data shape: {processed_data.shape}\")\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hashtag Analysis and Narrative Propagation Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashtagAnalyzer:\n",
    "    \"\"\"Comprehensive hashtag analysis for misinformation tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.hashtag_trends = None\n",
    "        self.narrative_clusters = None\n",
    "        \n",
    "    def analyze_hashtag_frequency(self):\n",
    "        \"\"\"Analyze hashtag frequency and trending patterns\"\"\"\n",
    "        # Flatten all hashtags\n",
    "        all_hashtags = []\n",
    "        for hashtag_list in self.data['hashtag_text']:\n",
    "            all_hashtags.extend(hashtag_list)\n",
    "        \n",
    "        # Count frequencies\n",
    "        hashtag_counts = Counter(all_hashtags)\n",
    "        \n",
    "        # Create trending analysis\n",
    "        trending_df = pd.DataFrame([\n",
    "            {'hashtag': tag, 'frequency': count, 'percentage': (count/len(all_hashtags))*100}\n",
    "            for tag, count in hashtag_counts.most_common(50)\n",
    "        ])\n",
    "        \n",
    "        self.hashtag_trends = trending_df\n",
    "        return trending_df\n",
    "    \n",
    "    def detect_coordinated_campaigns(self, time_window_hours=6, min_posts=10):\n",
    "        \"\"\"Detect potential coordinated misinformation campaigns\"\"\"\n",
    "        campaigns = []\n",
    "        \n",
    "        # Group by hashtag combinations\n",
    "        hashtag_combos = self.data['hashtags'].apply(lambda x: tuple(sorted(x)))\n",
    "        combo_groups = self.data.groupby(hashtag_combos)\n",
    "        \n",
    "        for combo, group in combo_groups:\n",
    "            if len(group) >= min_posts:\n",
    "                # Check if posts are clustered in time\n",
    "                time_diffs = group['timestamp'].diff().dt.total_seconds() / 3600\n",
    "                clustered_posts = (time_diffs <= time_window_hours).sum()\n",
    "                \n",
    "                if clustered_posts >= min_posts * 0.7:  # 70% of posts clustered\n",
    "                    campaign_info = {\n",
    "                        'hashtag_combo': combo,\n",
    "                        'post_count': len(group),\n",
    "                        'time_span_hours': (group['timestamp'].max() - group['timestamp'].min()).total_seconds() / 3600,\n",
    "                        'avg_engagement': group['engagement_score'].mean(),\n",
    "                        'platforms': group['platform'].unique().tolist(),\n",
    "                        'risk_score': self._calculate_campaign_risk(group)\n",
    "                    }\n",
    "                    campaigns.append(campaign_info)\n",
    "        \n",
    "        return pd.DataFrame(campaigns).sort_values('risk_score', ascending=False)\n",
    "    \n",
    "    def _calculate_campaign_risk(self, group):\n",
    "        \"\"\"Calculate risk score for potential misinformation campaign\"\"\"\n",
    "        risk_factors = {\n",
    "            'high_misinformation_risk': (group['misinformation_risk'] == 'high').sum() / len(group),\n",
    "            'low_credibility': (group['credibility_score'] < 0.3).sum() / len(group),\n",
    "            'not_fact_checked': (~group['fact_checked']).sum() / len(group),\n",
    "            'rapid_spread': (group['viral_score'] > group['viral_score'].quantile(0.8)).sum() / len(group)\n",
    "        }\n",
    "        \n",
    "        return sum(risk_factors.values()) / len(risk_factors)\n",
    "    \n",
    "    def create_narrative_clusters(self, n_clusters=8):\n",
    "        \"\"\"Cluster hashtags to identify narrative themes\"\"\"\n",
    "        # Create hashtag co-occurrence matrix\n",
    "        all_hashtags = set()\n",
    "        for hashtag_list in self.data['hashtag_text']:\n",
    "            all_hashtags.update(hashtag_list)\n",
    "        \n",
    "        hashtag_list = list(all_hashtags)\n",
    "        cooccurrence_matrix = np.zeros((len(hashtag_list), len(hashtag_list)))\n",
    "        \n",
    "        for hashtags in self.data['hashtag_text']:\n",
    "            for i, tag1 in enumerate(hashtag_list):\n",
    "                for j, tag2 in enumerate(hashtag_list):\n",
    "                    if tag1 in hashtags and tag2 in hashtags:\n",
    "                        cooccurrence_matrix[i][j] += 1\n",
    "        \n",
    "        # Apply clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        clusters = kmeans.fit_predict(cooccurrence_matrix)\n",
    "        \n",
    "        # Create cluster DataFrame\n",
    "        cluster_df = pd.DataFrame({\n",
    "            'hashtag': hashtag_list,\n",
    "            'cluster': clusters\n",
    "        })\n",
    "        \n",
    "        self.narrative_clusters = cluster_df\n",
    "        return cluster_df\n",
    "\n",
    "# Perform hashtag analysis\n",
    "analyzer = HashtagAnalyzer(processed_data)\n",
    "trending_hashtags = analyzer.analyze_hashtag_frequency()\n",
    "potential_campaigns = analyzer.detect_coordinated_campaigns()\n",
    "narrative_clusters = analyzer.create_narrative_clusters()\n",
    "\n",
    "print(\"üîç Hashtag Analysis Complete\")\n",
    "print(f\"üìà Top 10 trending hashtags:\")\n",
    "print(trending_hashtags.head(10))\n",
    "print(f\"\\n‚ö†Ô∏è  Detected {len(potential_campaigns)} potential coordinated campaigns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}